["Dear mlpack developers,\n\nthe OpenCV Machine Learning Library [1] is another location where C++\nimplementations of machine learning methods are collected. How do you\nposition yourself with respect to that library?\n\nE.g., [2] gives the impression that you are not going to wrap 3rd party\nmethods, like libsvm (as OpenCV does). Is this correct?\n\nBest regards,\nRobert\n\n[1] http://docs.opencv.org/modules/ml/doc/ml.html\n[2] http://trac.research.cc.gatech.edu/fastlab/ticket/122\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "Hello,\n\nis there an archive of this mailing list? The one on\nhttps://mailman.cc.gatech.edu/pipermail/mlpack/ is empty.\n\nBtw., I can recommend setting one up at http://gmane.org/. I found their\nsearch functionality useful, and one can also import a local (mbox)\narchive to initialize it.\n\nRobert\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Mon, Mar 11, 2013 at 04:17:14PM +0100, Jaelon wrote:\n> Dear Ryan,\n> \n> First of all I want to thank you for the fast response.\n> It kind of figured that I could just use double with a simple type\n> conversion, but was unsure about the influence on the results.\n> \n> However I am still left with the question what to do with the\n> \"vector<arma::Col<size_t> > states;\".\n> Is this the amount of values in one sequence of observed values or\n> is it the amount of possible states that it can be, or maybe even\n> something else entirely?\n> \n> In my case I get my data from video samples and the amount of values\n> for one video sample vary. I have already changed my code a bit so\n> that the values per sample are stored in a different column of the\n> \"vector<arma::Mat<double> >\", meaning that each column contains a\n> sequence of values for one video sample and the amount of rows is\n> equal to the amount of video samples used. I hope this is the way it\n> is supposed to be done, as I have no way of testing it until the my\n> problem with states is fixed.\n\nAh!  Sorry, I forgot to answer that question.\n\nThe states vector (vector<arma::Col<size_t> >) are the states which\ncorrespond to each observation (the observations vector,\nvector<arma::Mat<double> >).  When you train the HMM with Train(), it is\ntraining with labeled data.  So for each observation, you need to pass\nthe corresponding label.  That is, states[i] is the index of the hidden\nstate corresponding with observations[i].\n\nIn the example you gave, you were only using one hidden state, so the\nstates vector should be entirely filled with zeroes, and be the same\nlength as the observations vector.\n\nThank you for pointing out these issues.  Sometimes, when writing\ndocumentation, it is hard to know what people will find confusing.\n\n-- \nRyan Curtin       | \"So I got that going for me, which is nice.\"\nryan@igglybob.com |   - Carl Spackler\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "(missed reply-all a second ago)\n\nThat patch is somewhat similar to a mod I tried. Mine helped sometimes, but\nnot in all situations. Both problems still occur with r14883:\n\n$ ./gmm -i gmm_obs0.csv -g 5\n[DEBUG] Compiled with debugging symbols.\n[DEBUG] Iterations: 42\n[DEBUG] EMFit::Estimate(): initial clustering log-likelihood: 1.4249e+06\n\nerror: inv(): matrix appears to be singular\n\nterminate called after throwing an instance of 'std::runtime_error'\n  what():\nAborted (core dumped)\n\n$ ./gmm -i gmm_obs0.csv -g 8\n[DEBUG] Compiled with debugging symbols.\n[DEBUG] Iterations: 57\n[DEBUG] EMFit::Estimate(): initial clustering log-likelihood: -inf\n[DEBUG] GMM::Estimate(): Log-likelihood of trial 0 is -nan.\n[DEBUG] Iterations: 83\n[DEBUG] EMFit::Estimate(): initial clustering log-likelihood: -inf\n[DEBUG] GMM::Estimate(): Log-likelihood of trial 1 is -nan.\n[DEBUG] Iterations: 59\n[DEBUG] EMFit::Estimate(): initial clustering log-likelihood: -inf\n[DEBUG] GMM::Estimate(): Log-likelihood of trial 2 is -nan.\n^C\n\n~John\n\n\nOn Tue, Apr 9, 2013 at 2:06 PM, Ryan Curtin <gth671b@mail.gatech.edu> wrote:\n\n> On Tue, Apr 09, 2013 at 02:02:24PM -0400, Ryan Curtin wrote:\n> > I recently committed a fix which adds some small values to the\n> > covariance matrices so that they don't end up being entirely\n> > zero-valued.\n>\n> My mistake.  I hadn't committed this fix until just now (r148883).\n>\n> --\n> Ryan Curtin       | \"I am a meat popsicle.\"\n> ryan@igglybob.com |   - Korben Dallas\n> _______________________________________________\n> mlpack mailing list\n> mlpack@cc.gatech.edu\n> https://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n>\n<div dir=3D\"ltr\"><div style>(missed reply-all a second ago)<br></div><div><=\nbr></div>That patch is somewhat similar to a mod I tried. Mine helped somet=\nimes, but not in all situations. Both problems still occur with r14883:<br>\n\n<div class=3D\"gmail_quote\"><div dir=3D\"ltr\"><div><br></div><div><div class=\n=3D\"im\"><div>$ ./gmm -i gmm_obs0.csv -g 5</div></div><div>[DEBUG] Compiled =\nwith debugging symbols.</div>\n<div>[DEBUG] Iterations: 42</div><div>[DEBUG] EMFit::Estimate(): initial cl=\nustering log-likelihood: 1.4249e+06</div><div class=3D\"im\"><div><br></div><=\ndiv>error: inv(): matrix appears to be singular</div><div><br></div></div>\n\n<div>terminate called after throwing an instance of &#39;std::runtime_error=\n&#39;</div>\n<div>=A0 what(): =A0</div><div>Aborted (core dumped)</div></div><div><br></=\ndiv><div><div>$ ./gmm -i gmm_obs0.csv -g 8</div><div>[DEBUG] Compiled with =\ndebugging symbols.</div><div>[DEBUG] Iterations: 57</div><div>[DEBUG] EMFit=\n::Estimate(): initial clustering log-likelihood: -inf</div>\n\n\n<div>[DEBUG] GMM::Estimate(): Log-likelihood of trial 0 is -nan.</div><div>=\n[DEBUG] Iterations: 83</div><div>[DEBUG] EMFit::Estimate(): initial cluster=\ning log-likelihood: -inf</div><div>[DEBUG] GMM::Estimate(): Log-likelihood =\nof trial 1 is -nan.</div>\n\n\n<div>[DEBUG] Iterations: 59</div><div>[DEBUG] EMFit::Estimate(): initial cl=\nustering log-likelihood: -inf</div><div>[DEBUG] GMM::Estimate(): Log-likeli=\nhood of trial 2 is -nan.</div><div>^C</div></div><span class=3D\"HOEnZb\"><fo=\nnt color=3D\"#888888\"><div>\n\n<br></div><div>\n~John</div></font></span></div><div class=3D\"HOEnZb\"><div class=3D\"h5\"><div=\n class=3D\"gmail_extra\"><br><br><div class=3D\"gmail_quote\">On Tue, Apr 9, 20=\n13 at 2:06 PM, Ryan Curtin <span dir=3D\"ltr\">&lt;<a href=3D\"mailto:gth671b@=\nmail.gatech.edu\" target=3D\"_blank\">gth671b@mail.gatech.edu</a>&gt;</span> w=\nrote:<br>\n\n\n<blockquote class=3D\"gmail_quote\" style=3D\"margin:0 0 0 .8ex;border-left:1p=\nx #ccc solid;padding-left:1ex\"><div>On Tue, Apr 09, 2013 at 02:02:24PM -040=\n0, Ryan Curtin wrote:<br>\n&gt; I recently committed a fix which adds some small values to the<br>\n&gt; covariance matrices so that they don&#39;t end up being entirely<br>\n&gt; zero-valued.<br>\n<br>\n</div>My mistake. =A0I hadn&#39;t committed this fix until just now (r14888=\n3).<br>\n<span><font color=3D\"#888888\"><br>\n--<br>\nRyan Curtin =A0 =A0 =A0 | &quot;I am a meat popsicle.&quot;<br>\n<a href=3D\"mailto:ryan@igglybob.com\" target=3D\"_blank\">ryan@igglybob.com</a=\n> | =A0 - Korben Dallas<br>\n</font></span><div><div>_______________________________________________<br>\nmlpack mailing list<br>\n<a href=3D\"mailto:mlpack@cc.gatech.edu\" target=3D\"_blank\">mlpack@cc.gatech.=\nedu</a><br>\n<a href=3D\"https://mailman.cc.gatech.edu/mailman/listinfo/mlpack\" target=3D=\n\"_blank\">https://mailman.cc.gatech.edu/mailman/listinfo/mlpack</a><br>\n</div></div></blockquote></div><br></div>\n</div></div></div><br></div>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "Dear Mrs, Sir,\n\nI have only recently begun learning about machine learning and more \nspecific about hidden markov models as I need to use these.\nI have a project in C++ and the mlpack library seemed like the best \noption to use for training and using HMM. However I find the \ndocumentation about HMM somewhat vague on not entirely clear.\nThere is a piece of example code there but that doesn't work for me.\n\nHere is an overview of what I need to do:\nI have a program that throws me data on which i need to train the HMM. \nThe data contains discrete as well as continuous data. The data can vary \nbetween 4 and 27 variables being integers, floats and vector<floats>. If \nI am correct these are my observations as I know what these stand for \nand can extract them with ease.\nBut I don't need to use all of the different variables at once as I want \nto look at the performance when using every one of them on its own and \nthen expanding it to take more variables into account.\nFor now I want to look at the first variable.\nThis is a sequence of integer values ranging from 0 to 5 (thus \ndiscrete), these values are stored in a vector<int> . So here is what i \ntry to do:\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\nvector<arma::Mat<int> > handvector;\narma::Mat<int> handobservations;\nvector<arma::Col<size_t> > states;\nint hidden_states = 1; //leaving this on 1 for now\n\n  hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states, \ndistribution::DiscreteDistribution());\n     for(uint i = 0; i<hands.size();i++)  //hands is a vector<int> that \nis created earlier in the code\n     {\n         handobservations << hands[i] << arma::endr;\n         handvector.push_back(handobservations);\n         handobservations.clear();\n     }\n\nhandhmm.Train(handvector, states);\n/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n\nNow the problem with this is the Train member function won't accept \nvector<arma::Mat<int> >, it only seems to accept \nvector<arma::Mat<double> >.\nAlso states is left empty for this and that isn't accepted either. So \nwhat needs to be put in states and is there a way to make train work \nwith vector<arma::Mat<int> >?\n\nAny help or a more extended example than in the doc will be greatly \nappreciated.\n\nWith kind regards,\nJoery\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Tue, Apr 09, 2013 at 12:37:22PM +0800, zhijie he wrote:\n> Hi,\n> \n> I am Zhijie He, a master candidate at National University of Singapore. My\n> research interest is natural language processing and machine learning. I\n> will gradate this May, so I have a lot of time to do the GSoC project.\n> \n> I am interested in developing a collaborative filtering package for mlpack.\n> I was wondering before submit my application, do I have to do some other\n> things like submit a patch for mlpack? Do you have other requirements for\n> the applicants?\n\nHello Zhijie (and Salman who had nearly the same question),\n\nThe collaborative filtering project is as much of a research project as\nit is an implementation project.  As the description says, the project\nentails researching what the state-of-the-art is in CF and implementing\nit, with an emphasis on scalability.  It is a somewhat open-ended\nproject, so a good application for that project should already have CF\ndomain knowledge and ideas for what exactly could be implemented.\n\nYou are welcome to submit patches to mlpack if you find bugs, but that\nis not necessary for a GSoC application.\n\nHopefully this information is helpful.\n\nThanks,\n\nRyan\n\n-- \nRyan Curtin       | \"I am the luckiest man alive!\"\nryan@igglybob.com |   - General Borzov\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On 03/11/2013 03:41 PM, Ryan Curtin wrote:\n> On Mon, Mar 11, 2013 at 11:10:56AM +0100, Jaelon wrote:\n>> Dear Mrs, Sir,\n>>\n>> I have only recently begun learning about machine learning and more\n>> specific about hidden markov models as I need to use these.\n>> I have a project in C++ and the mlpack library seemed like the best\n>> option to use for training and using HMM. However I find the\n>> documentation about HMM somewhat vague on not entirely clear.\n>> There is a piece of example code there but that doesn't work for me.\n>>\n>> Here is an overview of what I need to do:\n>> I have a program that throws me data on which i need to train the\n>> HMM. The data contains discrete as well as continuous data. The data\n>> can vary between 4 and 27 variables being integers, floats and\n>> vector<floats>. If I am correct these are my observations as I know\n>> what these stand for and can extract them with ease.\n>> But I don't need to use all of the different variables at once as I\n>> want to look at the performance when using every one of them on its\n>> own and then expanding it to take more variables into account.\n>> For now I want to look at the first variable.\n>> This is a sequence of integer values ranging from 0 to 5 (thus\n>> discrete), these values are stored in a vector<int> . So here is\n>> what i try to do:\n>>\n>> ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n>> vector<arma::Mat<int> > handvector;\n>> arma::Mat<int> handobservations;\n>> vector<arma::Col<size_t> > states;\n>> int hidden_states = 1; //leaving this on 1 for now\n>>\n>>   hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states,\n>> distribution::DiscreteDistribution());\n>>      for(uint i = 0; i<hands.size();i++)  //hands is a vector<int>\n>> that is created earlier in the code\n>>      {\n>>          handobservations << hands[i] << arma::endr;\n>>          handvector.push_back(handobservations);\n>>          handobservations.clear();\n>>      }\n>>\n>> handhmm.Train(handvector, states);\n>> /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n>>\n>> Now the problem with this is the Train member function won't accept\n>> vector<arma::Mat<int> >, it only seems to accept\n>> vector<arma::Mat<double> >.\n>> Also states is left empty for this and that isn't accepted either.\n>> So what needs to be put in states and is there a way to make train\n>> work with vector<arma::Mat<int> >?\n>>\n>> Any help or a more extended example than in the doc will be greatly\n>> appreciated.\n> Hello Joery,\n>\n> The Train() function accepts vector<arma::Mat<double> > regardless of\n> the distribution type.  For the discrete distribution, casting the\n> integers to doubles should work, but this is not an optimal solution:\n>\n>    arma::Mat<double> handobservations;\n>    for (uint i = 0; i < hands.size(); i++)\n>    {\n>      handobservations << (double) hands[i] << arma::endr;\n>      handvector.push_back(handobservations);\n>      handobservations.clear();\n>    }\n>\n> In the coming days I will refactor the HMM class so that for\n> DiscreteDistribution, arma::Mat<int> will be accepted (as it should be).\n> This will be slightly involved because it means each distribution type\n> will have to define its own observation type (arma::Mat<int> for\n> DiscreteDistribution, arma::Mat<double> for GaussianDistribution, and so\n> forth).\n>\n> I will also write more complete documentation and a tutorial for HMMs,\n> and then I'll let you know when that is all done.  In the mean time, the\n> fix I proposed above should work.  If it doesn't, or you have any other\n> questions, let me know.\nDear Ryan,\n\nFirst of all I want to thank you for the fast response.\nIt kind of figured that I could just use double with a simple type \nconversion, but was unsure about the influence on the results.\n\nHowever I am still left with the question what to do with the \n\"vector<arma::Col<size_t> > states;\".\nIs this the amount of values in one sequence of observed values or is it \nthe amount of possible states that it can be, or maybe even something \nelse entirely?\n\nIn my case I get my data from video samples and the amount of values for \none video sample vary. I have already changed my code a bit so that the \nvalues per sample are stored in a different column of the \n\"vector<arma::Mat<double> >\", meaning that each column contains a \nsequence of values for one video sample and the amount of rows is equal \nto the amount of video samples used. I hope this is the way it is \nsupposed to be done, as I have no way of testing it until the my problem \nwith states is fixed.\n\n", "Hi I am a student at the University of Calgary and interested in working =\non one of your projects for GSoC.  I will be in the middle of my second =\nyear but I do very well on assignments so I am still at early stages of =\nmy degree but am skilled at programming.   I have done work in Java, =\npython, assembly (SPARC architecture) and did some exploration of =\nobjective C last summer.=20\n\nI was intrigued by your improvement of tree traversers suggestion, I am =\ncurrently in a course that has a component on trees and so did some work =\nwith them.  I am interested in learning about and learning machine =\nlearning however I have never programmed in C++.  Do you think this is =\nwithin my skill set that I have laid out?\n\nThank you,\nDaniel=\n<html><head><meta http-equiv=3D\"Content-Type\" content=3D\"text/html =\ncharset=3Dus-ascii\"></head><body style=3D\"word-wrap: break-word; =\n-webkit-nbsp-mode: space; -webkit-line-break: after-white-space; \"><div =\nstyle=3D\"margin: 0px; font-size: 13px; font-family: Arial; color: =\nrgb(35, 35, 35); \">Hi I am a student at the University of Calgary and =\ninterested in working on one of your projects for GSoC. &nbsp;I will be =\nin the middle of my second year but I do very well on assignments so I =\nam still at early stages of my degree but am skilled at programming. =\n&nbsp; I have done work in Java, python, assembly (SPARC architecture) =\nand did some exploration of objective C last summer.&nbsp;</div><div =\nstyle=3D\"margin: 0px; font-size: 13px; font-family: Arial; color: =\nrgb(35, 35, 35); \"><br></div><div style=3D\"margin: 0px; font-size: 13px; =\nfont-family: Arial; color: rgb(35, 35, 35); \">I was intrigued by your =\nimprovement of tree traversers suggestion, I am currently in a course =\nthat has a component on trees and so did some work with them. &nbsp;I am =\ninterested in learning about and learning machine learning however I =\nhave never programmed in C++. &nbsp;Do you think this is within my skill =\nset that I have laid out?</div><div style=3D\"margin: 0px; font-size: =\n13px; font-family: Arial; color: rgb(35, 35, 35); \"><br></div><div =\nstyle=3D\"margin: 0px; font-size: 13px; font-family: Arial; color: =\nrgb(35, 35, 35); \">Thank you,</div><div style=3D\"margin: 0px; font-size: =\n13px; font-family: Arial; color: rgb(35, 35, 35); =\n\">Daniel</div></body></html>=\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "Hi,\n\nI am Zhijie He, a master candidate at National University of Singapore. My\nresearch interest is natural language processing and machine learning. I\nwill gradate this May, so I have a lot of time to do the GSoC project.\n\nI am interested in developing a collaborative filtering package for mlpack.\nI was wondering before submit my application, do I have to do some other\nthings like submit a patch for mlpack? Do you have other requirements for\nthe applicants?\n\nThank you.\n\nBest Regards,\nZhijie\n<div dir=3D\"ltr\">Hi,<div><br></div><div style>I am Zhijie He, a master cand=\nidate at National University of Singapore. My research interest is natural =\nlanguage processing and machine learning. I will gradate this May, so I hav=\ne a lot of time to do the GSoC project.</div>\n<div style><br></div><div style>I am interested in developing a collaborati=\nve filtering package for mlpack. I was wondering before submit my applicati=\non, do I have to do some other things like submit a patch for mlpack? Do yo=\nu have other requirements for the applicants?</div>\n<div style><br></div><div style>Thank you.</div><div style><br></div><div s=\ntyle>Best Regards,</div><div style>Zhijie</div></div>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "On Tue, Apr 09, 2013 at 09:04:59PM -0600, Daniel Bell wrote:\n> oh ok thank you, I guess at my current level it is just outside my\n> reach.  I am in my first algorithm course.  could you recommend\n> something similar that might be more in my current skill set?\n\nIf you are deadset on doing statistical machine learning implementation\nand none of the other less machine-learning-y projects are appealing to\nyou, I suggest that your best bet is learning C++ and a fairly simple\nmachine learning method (maybe decision trees of some sort -- which are\ndifferent than the dual-tree algorithms I mentioned earlier; or maybe\nsome rather simple nonlinear dimensionality reduction method such as\nIsoMap or LLE).  Keep in mind that mlpack is looking for fast\nalgorithms, so the off-the-shelf IsoMap algorithm (for example) may not\nbe as asymptotically fast as some more recently published improvement on\nthe algorithm (in the case of IsoMap I am not certain if one exists).\n\n-- \nRyan Curtin       | \"I'm going to be on television!\"\nryan@igglybob.com |   - Sara Goldfarb\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "Hi All-\n\nI'm trying to use mlpack's GMM with some data I've got. I'm not so familiar\nwith the statistical tools used here as I should be, so I've run into some\nproblems that I'm having trouble debugging on my own:\n\n- First, I often get \"error: inv(): matrix appears to be singular\" during\nestimation. It appears that during estimation, one (or more) rows and\ncolumns of a covariance matrix become 0, and I think this causes it to\nbecome non-invertible.\n\n- Second, in cases when estimation completes, I often end up with means,\nweights and covariances which are all -nan.\n\nI'm not sure whether I'm mis-using the tool or I've got funny data which\nneed to be conditioned. It's six-dimensional, values less than 1.0 and one\nof the features is very often zero. (I'm wondering if that last bit means\nthat one good gaussian would be zero mean and zero stdev, resulting in a\ndegenerate covariance matrix -- though I don't know enough stat and linear\nalgebra to work this out.) Can someone give some advice?\n\nI've posted a small sub-set of my data which can trigger these problems:\nwww.cs.columbia.edu/~jdd/gmm_obs0.csv\n\nIf I run \"./gmm -i gmm_obs0.csv -g 5\" I can get the first problem. Changing\nthe number of gaussians to 8 results in the second problem.\n\nThanks in advance,\nJohn\n<div dir=3D\"ltr\"><div style>Hi All-</div><div style><br></div><div style>I&=\n#39;m trying to use mlpack&#39;s GMM with some data I&#39;ve got. I&#39;m n=\not so familiar with the statistical tools used here as I should be, so I&#3=\n9;ve run into some problems that I&#39;m having trouble debugging on my own=\n:</div>\n\n<div style><br></div><div style>- First, I often get &quot;error: inv(): ma=\ntrix appears to be singular&quot; during estimation. It appears that during=\n estimation, one (or more) rows and columns of a covariance matrix become 0=\n, and I think this causes it to become non-invertible.</div>\n\n<div style><br></div><div style>- Second, in cases when estimation complete=\ns, I often end up with means, weights and covariances which are all -nan.</=\ndiv><div style><br></div><div style>I&#39;m not sure whether I&#39;m mis-us=\ning the tool or I&#39;ve got funny data which need to be conditioned. It&#3=\n9;s six-dimensional, values less than 1.0 and one of the features is very o=\nften zero. (I&#39;m wondering if that last bit means that one good gaussian=\n would be zero mean and zero stdev, resulting in a degenerate covariance ma=\ntrix -- though I don&#39;t know enough stat and linear algebra to work this=\n out.) Can someone give some advice?</div>\n\n<div style><br></div><div style>I&#39;ve posted a small sub-set of my data =\nwhich can trigger these problems:</div><a href=3D\"http://www.cs.columbia.ed=\nu/~jdd/gmm_obs0.csv\">www.cs.columbia.edu/~jdd/gmm_obs0.csv</a><br><div><br>\n\n</div><div style>If I run &quot;./gmm -i gmm_obs0.csv -g 5&quot; I can get =\nthe first problem. Changing the number of gaussians to 8 results in the sec=\nond problem.</div><div style><br></div><div style>Thanks in advance,</div>\n\n<div style>John<br></div></div>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "On Fri, Mar 15, 2013 at 01:14:29PM +0100, Jaelon wrote:\n>    Me again,\n> \n>    I now understand how the observations are supposed to be filled in and\n>    changed that in my code.\n>    As you mentioned I just needed a transpose of the arma::mat matrices to\n>    have the observation vector filled in correctly. However it is still\n>    giving me the same error.\n>    Now I have tried to get it working with a simple piece of hardcoded\n>    data as follows:\n> \n>        vector<arma::Mat<double> > handvector;\n>        arma::Mat<double> observations;\n>        int hidden_states = 5;\n>        size_t observ_dimension;\n>        vector<distribution::DiscreteDistribution> emissions;\n>        arma::mat transitions;\n> \n>        observations << 5 << 4 << 5 << 5 << arma::endr;\n>        observations << 2 << 1 << 1 << 2 << arma::endr;\n>        handvector.push_back(observations);\n> \n>        hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states,\n>    distribution::DiscreteDistribution(observations.n_rows));\n> \n>        printf(\"Ready for training\\n\");\n>        handhmm.Train(handvector); //the error comes from calling training\n>    when looking at my output\n>        printf(\"Done training model\\n\");\n>        observ_dimension = handhmm.Dimensionality();\n>        printf(\"Retrieved dimensionality\\n\");\n>        emissions = handhmm.Emission();\n>        printf(\"Retrieved emission matrix\\n\");\n>        transitions = handhmm.Transition();\n>        printf(\"Retrieved transition matrix\");\n> \n>    As you see I used 4 columns with 2 datapoints for each observation\n>    while using only one sequence.\n>    This is a very simple example that is a fictional representation of\n>    what I put into the observations (when I would use 1 video with a very\n>    small amount of extracted data).\n>    I initialize the hmm (handhmm) with 5 hidden states (this is just a\n>    random number I chose as the number of hidden states will be unknown in\n>    the real case) and a discrete distribution for 2 observations.\n> \n>    Maybe this example can clarify what I am doing and give you insight in\n>    what I might be doing wrong.\n>    It think it might also be helpful to others to have such an example in\n>    the documentation.\n> \n>    With kind regards,\n>    Joery\n\nSorry for the slow reply; things have been busy for me.\n\nThe problem is here:\n\n> hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states,\n>    distribution::DiscreteDistribution(observations.n_rows));\n\nThe constructor of DiscreteDistribution is defined as (from\ncore/dists/discrete_distribution.hpp):\n\n> /**\n>   * Define the discrete distribution as having numObservations possible\n>   * observations.  The probability in each state will be set to (1 /\n>   * numObservations).\n>   *\n>   * @param numObservations Number of possible observations this\n>   * distribution\n>   *    can have.\n>   */\n>  DiscreteDistribution(const size_t numObservations);\n\nYou are giving observations.n_rows, which is the dimensionality of the\nobservations, and not the number of possible observations.  Your\nobservations vector contains observation '5', while\nDiscreteDistribution() only expects to see one observation.\n\nTo correct this, the constructor parameter to DiscreteDistribution()\nshould be one greater than the highest-numbered state that is observed\n(because 0 is also a valid state):\n\n> hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states,\n>    distribution::DiscreteDistribution(6));\n\nI have modified DiscreteDistribution to provide much better error output\nwhen compiled with debugging symbols (-DDEBUG).  With this change your\nexample code gives:\n\n----\nReady for training\n[DEBUG] DiscreteDistribution::Probability(): received observation 2;\nobservation must be in [0, 1] for this distribution.\n\nerror: Mat::operator(): out of bounds\n\nterminate called after throwing an instance of 'std::logic_error'\n  what():  \nAborted\n----\n\n>    Edit: I have also figured out that\n>    observations << 5 << 4 << 5 << 5 << arma::endr;\n>    observations << 2 << 1 << 1 << 2 << arma::endr;\n>    writes to the first row 2 times so I changed that to\n>    observations << 5 << 4 << 5 << 5 << arma::endr << 2 << 1 << 1 << 2 <<\n>    arma::endr;\n>    However that doesn't fix the problem (only brought to my intention that\n>    it is better to just initialize the arma::mat with a given size and\n>    then setting the elements individually).\n\nThis is true, but be aware that the DiscreteDistribution class currently\nonly supports single-dimensional observations.  If you have\nmulti-dimensional discrete data, you could convert it into\none-dimensional data kind of like this:\n\n  state (0, 0) --> state 0\n  state (0, 1) --> state 1\n  state (0, 2) --> state 2\n  state (1, 0) --> state 3\n  state (1, 1) --> state 4\n  state (1, 2) --> state 5\n\nI hope this information is helpful, and again, sorry for the slow\nresponse.\n\n-- \nRyan Curtin       | \"For more enjoyment and greater efficiency,\nryan@igglybob.com | consumption is being standardized.\"\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "Hey,\n\nI am currently working with GMM and it worked for two models with decent \nresults up to now.\nHowever when attempting to train for a third model I get mean, \ncovariance and transmission matrices filled with NaN.\nMy input data has 15 dimensions consisting of values that range between \n0 and 4.\nMy guess is that excessive appearance of the value zero is causing the \nproblem and thus I tried adding a very small value to every datapoint, \nbut this did not solve the problem.\n\nI am training 7 models with data like explained above. 2 of them always \ngive me NaN matrices and take way longer to train so something is going \nwrong with the training.\nCould you give me an possible reason for this problem and/or point me in \nthe direction to solving this, I have been stuck several hours on this now).\n\nPS. I updated from the repository about 6hours ago (version r14885)\n\nWith many thanks,\nJoery\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On 03/19/2013 07:56 PM, Ryan Curtin wrote:\n> On Fri, Mar 15, 2013 at 01:14:29PM +0100, Jaelon wrote:\n>>     Me again,\n>>\n>>     I now understand how the observations are supposed to be filled in and\n>>     changed that in my code.\n>>     As you mentioned I just needed a transpose of the arma::mat matrices to\n>>     have the observation vector filled in correctly. However it is still\n>>     giving me the same error.\n>>     Now I have tried to get it working with a simple piece of hardcoded\n>>     data as follows:\n>>\n>>         vector<arma::Mat<double> > handvector;\n>>         arma::Mat<double> observations;\n>>         int hidden_states = 5;\n>>         size_t observ_dimension;\n>>         vector<distribution::DiscreteDistribution> emissions;\n>>         arma::mat transitions;\n>>\n>>         observations << 5 << 4 << 5 << 5 << arma::endr;\n>>         observations << 2 << 1 << 1 << 2 << arma::endr;\n>>         handvector.push_back(observations);\n>>\n>>         hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states,\n>>     distribution::DiscreteDistribution(observations.n_rows));\n>>\n>>         printf(\"Ready for training\\n\");\n>>         handhmm.Train(handvector); //the error comes from calling training\n>>     when looking at my output\n>>         printf(\"Done training model\\n\");\n>>         observ_dimension = handhmm.Dimensionality();\n>>         printf(\"Retrieved dimensionality\\n\");\n>>         emissions = handhmm.Emission();\n>>         printf(\"Retrieved emission matrix\\n\");\n>>         transitions = handhmm.Transition();\n>>         printf(\"Retrieved transition matrix\");\n>>\n>>     As you see I used 4 columns with 2 datapoints for each observation\n>>     while using only one sequence.\n>>     This is a very simple example that is a fictional representation of\n>>     what I put into the observations (when I would use 1 video with a very\n>>     small amount of extracted data).\n>>     I initialize the hmm (handhmm) with 5 hidden states (this is just a\n>>     random number I chose as the number of hidden states will be unknown in\n>>     the real case) and a discrete distribution for 2 observations.\n>>\n>>     Maybe this example can clarify what I am doing and give you insight in\n>>     what I might be doing wrong.\n>>     It think it might also be helpful to others to have such an example in\n>>     the documentation.\n>>\n>>     With kind regards,\n>>     Joery\n> Sorry for the slow reply; things have been busy for me.\n>\n> The problem is here:\n>\n>> hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states,\n>>     distribution::DiscreteDistribution(observations.n_rows));\n> The constructor of DiscreteDistribution is defined as (from\n> core/dists/discrete_distribution.hpp):\n>\n>> /**\n>>    * Define the discrete distribution as having numObservations possible\n>>    * observations.  The probability in each state will be set to (1 /\n>>    * numObservations).\n>>    *\n>>    * @param numObservations Number of possible observations this\n>>    * distribution\n>>    *    can have.\n>>    */\n>>   DiscreteDistribution(const size_t numObservations);\n> You are giving observations.n_rows, which is the dimensionality of the\n> observations, and not the number of possible observations.  Your\n> observations vector contains observation '5', while\n> DiscreteDistribution() only expects to see one observation.\n>\n> To correct this, the constructor parameter to DiscreteDistribution()\n> should be one greater than the highest-numbered state that is observed\n> (because 0 is also a valid state):\n>\n>> hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states,\n>>     distribution::DiscreteDistribution(6));\n> I have modified DiscreteDistribution to provide much better error output\n> when compiled with debugging symbols (-DDEBUG).  With this change your\n> example code gives:\n>\n> ----\n> Ready for training\n> [DEBUG] DiscreteDistribution::Probability(): received observation 2;\n> observation must be in [0, 1] for this distribution.\n>\n> error: Mat::operator(): out of bounds\n>\n> terminate called after throwing an instance of 'std::logic_error'\n>    what():\n> Aborted\n> ----\n>\n>>     Edit: I have also figured out that\n>>     observations << 5 << 4 << 5 << 5 << arma::endr;\n>>     observations << 2 << 1 << 1 << 2 << arma::endr;\n>>     writes to the first row 2 times so I changed that to\n>>     observations << 5 << 4 << 5 << 5 << arma::endr << 2 << 1 << 1 << 2 <<\n>>     arma::endr;\n>>     However that doesn't fix the problem (only brought to my intention that\n>>     it is better to just initialize the arma::mat with a given size and\n>>     then setting the elements individually).\n> This is true, but be aware that the DiscreteDistribution class currently\n> only supports single-dimensional observations.  If you have\n> multi-dimensional discrete data, you could convert it into\n> one-dimensional data kind of like this:\n>\n>    state (0, 0) --> state 0\n>    state (0, 1) --> state 1\n>    state (0, 2) --> state 2\n>    state (1, 0) --> state 3\n>    state (1, 1) --> state 4\n>    state (1, 2) --> state 5\n>\n> I hope this information is helpful, and again, sorry for the slow\n> response.\n>\nThx for the feedback, it explains and clarifies a lot to me.\nI have been digging into the code myself and wondered about the \ndimensionality that is hardcoded on 1 and also about the possibility to \nmake one observation value of the vectors I have for each observation,\nI also started to implement the hmm algorithms myself today.\nHowever due to your answer I don't think there is any need to continue \nwith my own implementation, for which I am thankful as well :p .\n\nI can get the simple example running now and will try to do some more \nadvanced test tomorrow.\nOne more question now though: Is the download on the website the one \nwith the updated Distribution code, as it states \"released februari 8\"\n\nAgain many thanks,\n\nJoery\n\n", "On Tue, Apr 09, 2013 at 09:27:42PM +0530, Pramanshu Rajput wrote:\n> I am an undergraduate student at Indian Institute of Technology Roorkee,\n> India\n> \n> I want to know about the possibility of project on adding new features to\n> mlpack including Restricted Boltzmann Machines and some Genetic Algorithms\n> ranging from very basic like Simulated Annealing, (Mu,Lambda) Evolution\n> Strategy to some of the more advanced algorithms.\n\nGenetic algorithms are entertainingly slow, so I am not sure how useful\nthey are in the context of mlpack, which aims to be a fast library.\nAlso, simulated annealing and genetic algorithms are not the same thing,\nbut they are both global optimization strategies (in some sense).\n\nYou are not restricted to only the ideas in the Ideas List (as it says\non that page), but if you are considering doing some other project, you\nwill have to detail it in your proposal.  Your proposal will have to\nconvince us that your idea is a fast algorithm that can be implemented\nwithout additional library dependencies, that you know the algorithm\ninside-out and can implement it efficiently, other implementations that\nyou seek to outperform, and then how it will fit into the existing\nmlpack codebase (both from the perspective of code cleanliness and also\nthe bigger picture of how all mlpack algorithms fit together).\n\nHopefully this is helpful.  I was not entirely sure how to answer your\nquestion because it was a little vague.\n\n-- \nRyan Curtin       | \"The mighty hand of vengeance, sent down to strike\nryan@igglybob.com | the unroadworthy!\"  - The Nightrider\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Tue, Apr 09, 2013 at 08:16:43PM -0600, Daniel Bell wrote:\n> I was intrigued by your improvement of tree traversers suggestion, I\n> am currently in a course that has a component on trees and so did some\n> work with them.  I am interested in learning about and learning\n> machine learning however I have never programmed in C++.  Do you think\n> this is within my skill set that I have laid out?\n\nThe tree traversal project is a particularly difficult one which expects\nrigorous C++ knowledge.  This does not mean that you could not do it,\nbut in slightly more detail:\n\nMany of the algorithms in mlpack are fast because they are tree-based\nbranch-and-bound algorithms.  If you don't know about those you can read\nthem about them online; search for something like 'nearest neighbor' and\n'kd-trees'.\n\nIn addition, there are many different possible types of trees and many\ndifferent machine learning algorithms.  mlpack uses C++ templates to\nprovide abstract traversals and algorithms which can work with any type\nof tree (assuming the user implements the tree with the proper methods).\nThis is actually very neat and to my knowledge there is no other library\nwhich implements anything even remotely similar.\n\nHowever, the functionality is still somewhat experimental -- which is\nwhy the tree abstractions haven't been advertised as a main selling\npoint.  The branch and bound type algorithms to which I was referring to\nearlier depend heavily on tight bounds.  So improving the effectiveness\nof these tree traversals will depend on the implementation of the bounds\nin the algorithms and the efficiency of those bound calculations (as\nwell as other calculations).\n\nSo the project would involve both analyzing the existing traversals for\neither algorithmic or implementational improvements as well as examining\nthe existing algorithms.\n\nI could talk about this all day, given that it is my primary research\ninterest, but I'll spare the gory details for now.\n\ntl;dr: if you're a second-year undergraduate without heavy algorithmic\nknowledge and you're interested in this project, you probably are best\nserved by putting in 30 hours a week reading papers both on dual-tree\nalgorithms and learning C++ inside-out (metaprogramming does happen in\nthe mlpack world of trees).  So, I'm not going to say it's impossible,\nbut there's a reason its difficulty is rated a 9/10...\n\n-- \nRyan Curtin       | \"...I still don't know what it means.\"\nryan@igglybob.com |   - Rigby Reardon\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "oh ok thank you, I guess at my current level it is just outside my =\nreach.  I am in my first algorithm course.  could you recommend =\nsomething similar that might be more in my current skill set?\n\nThank you,\nDaniel\n\n\nOn 2013-04-09, at 8:59 PM, Ryan Curtin <gth671b@mail.gatech.edu> wrote:\n\n> On Tue, Apr 09, 2013 at 08:16:43PM -0600, Daniel Bell wrote:\n>> I was intrigued by your improvement of tree traversers suggestion, I\n>> am currently in a course that has a component on trees and so did =\nsome\n>> work with them.  I am interested in learning about and learning\n>> machine learning however I have never programmed in C++.  Do you =\nthink\n>> this is within my skill set that I have laid out?\n>=20\n> The tree traversal project is a particularly difficult one which =\nexpects\n> rigorous C++ knowledge.  This does not mean that you could not do it,\n> but in slightly more detail:\n>=20\n> Many of the algorithms in mlpack are fast because they are tree-based\n> branch-and-bound algorithms.  If you don't know about those you can =\nread\n> them about them online; search for something like 'nearest neighbor' =\nand\n> 'kd-trees'.\n>=20\n> In addition, there are many different possible types of trees and many\n> different machine learning algorithms.  mlpack uses C++ templates to\n> provide abstract traversals and algorithms which can work with any =\ntype\n> of tree (assuming the user implements the tree with the proper =\nmethods).\n> This is actually very neat and to my knowledge there is no other =\nlibrary\n> which implements anything even remotely similar.\n>=20\n> However, the functionality is still somewhat experimental -- which is\n> why the tree abstractions haven't been advertised as a main selling\n> point.  The branch and bound type algorithms to which I was referring =\nto\n> earlier depend heavily on tight bounds.  So improving the =\neffectiveness\n> of these tree traversals will depend on the implementation of the =\nbounds\n> in the algorithms and the efficiency of those bound calculations (as\n> well as other calculations).\n>=20\n> So the project would involve both analyzing the existing traversals =\nfor\n> either algorithmic or implementational improvements as well as =\nexamining\n> the existing algorithms.\n>=20\n> I could talk about this all day, given that it is my primary research\n> interest, but I'll spare the gory details for now.\n>=20\n> tl;dr: if you're a second-year undergraduate without heavy algorithmic\n> knowledge and you're interested in this project, you probably are best\n> served by putting in 30 hours a week reading papers both on dual-tree\n> algorithms and learning C++ inside-out (metaprogramming does happen in\n> the mlpack world of trees).  So, I'm not going to say it's impossible,\n> but there's a reason its difficulty is rated a 9/10...\n>=20\n> --=20\n> Ryan Curtin       | \"...I still don't know what it means.\"\n> ryan@igglybob.com |   - Rigby Reardon\n\n\n", "On Thu, Feb 28, 2013 at 05:09:56PM -0500, Audrius Stundzia wrote:\n>    Hello mlpack mailing list members,\n> \n>    As this is my first post to the mailing list, my appreciation to the\n>    developers for creating the wonderful mlpack package.\n\nHello Audrius!\n\nI am glad you find mlpack useful.\n\n>    I have a question regarding mlpack::neighbor::NeighborSearch\n> \n>    I am wondering if it is possible to perform a dynamic kNN search in\n>    mlpack efficiently solving the all-NN problem repeatedly with a\n>    changing point set S.\n> \n>    As far as I know, all previously described methods handle only the\n>    inserting and deleting of points between queries or the case of points\n>    moving along a predetermined trajectories. This is different from the\n>    dynamic problem I have in mind, wherein the points move slowly along\n>    trajectories, but that these trajectories are not known in advance as\n>    in the case of an optimization problem.\n> \n>    The idea is that if the changes between point positions in subsequent\n>    calls to the the mlpack::neighbor::AllkNN search algorithm are\n>    sufficiently small, updating the tree should be faster than rebuilding\n>    it from scratch. The mlpack algorithm would takes a S_i and a\n>    corresponding tree T_i and update the tree to T_i+1 corresponding to a\n>    new set of points S_i+1. A caveat is that the subsequent sets {S_i} and\n>    {S_i+1} will, in general, contain an similar, but not exactly the same,\n>    number of points.\n> \n>    Any insight regarding this question would be most appreciated.\n\nI approached a problem like this once; at the end of each AllkNN search,\nwe were finding an incremental transformation matrix and multiplying the\npoints by that, then recalculating nearest neighbors.  There are several\nways to do this:\n\n  (1) \n\nInstead of finding k nearest neighbors, find a * k nearest neighbors\nwhere a is some integer (I chose 10).  Then apply the transformation\nmatrix and do a brute-force search in the a * k nearest neighbors of\neach point.  Every now and then, the tree gets fully rebuilt.  This\ngives approximate solutions, though.  This is what we chose to do.\n\n  (2)\n\nFor each point x in S_i, you have some corresponding point x' in\nS_{i + 1}.  It seems like, in general, || x - x' || is small.\n\nEach BinarySpaceTree<> node has a corresponding HRectBound<>, which is\nthe hyperrectangle bound that represents the enclosing hyperrectangle of\nall the points in the node.\n\nYou could loop over each point in S_{i + 1}, find the BinarySpaceTree<>\nnode it is a part of, and then expand the HRectBound associated with\nthat node accordingly (using operator|=()).  You'd also need to expand\nthe HRectBounds of any parent nodes.\n\nThis would mean your BinarySpaceTree is no longer a space partitioning\ntree -- the nodes would be overlapping.  For AllkNN this is not a\nproblem; it will still work, but it may affect the runtime adversely,\nespecially as the bounding hyperrectangles become larger and larger.  In\nthe limit case you would get a tree with no discriminatory capabilities\nbecause all the bounding hyperrectangles are huge.\n\n  (3)\n\nYou could avoid modifying the hyperrectangles associated with each tree\nnode by taking each point x' and finding which node it fits into, and\nplacing it in that node (whereas x may have been in a different node).\nYou could also split a node if it now has more points than the leafSize,\nand delete nodes when they have no points in them.\n\nI am not sure about the running time of this approach, but your tree\nwould still be valid for any number of iterations and it would probably\nstill run all-NN search quickly (as opposed to approach (2)).\n\n----\n\nAre these ideas helpful?  I think I understood the problem.\n\nAs long as you have some sort of valid tree structure with valid\nbounding rectangles, the NeighborSearch class will find the correct,\nexact nearest neighbors.\n\n-- \nRyan Curtin       | \"Think!  What to do!\"\nryan@igglybob.com |   - The Master\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Thu, Mar 14, 2013 at 12:29:53PM +0100, Jaelon wrote:\n> Hey,\n> \n>  I have yet another question about the hmm from mlpack, more precisely\n>  about the syntax for observations.\n>  I wil be using the \"Train (const std::vector< arma::mat > &dataSeq)\"\n>  thus without the labeled data so the Baum-welch algorithm will be used.\n>  But when using it with dataseq filled up with data that is observable i\n>  get an error: Mat::operator(): out of bounds.\n>  Here is what I put in there:\n>    * I have a vectors of integers (which i convert to double for now)\n>      for each timestamp, thus lets say one vector for each frame of a\n>      video.\n>    * These vectors are of variable length (1 or 2)\n>    * There are multiple video's to train the hmm (obviously :p)\n>    * Now I use the dataseq as a vector containing the data from all\n>      videos. Thus lets say I use 5 videos, then the dataseq will have 5\n>      columns whereby each column contains an arma::mat. This arma::mat\n>      contains n rows (with n being the number of frames in the\n>      corresponding video), each row in itself then contains the actual\n>      vector data that I observed (thus 1 or 2 elements on each row).\n> \n>  Is this the way that it has to be used or am I using it wrong and do I\n>  need to put the data per video in dataseq and thus execute the Train\n>  method for each video.\n>  I would like to have some more information about how to use the dataseq\n>  in that function and hope that my explanation above is understandable\n>  as it is kind of hard to explain.\n\nThe documentation here is somewhat unclear, and I will be fixing that\nsoon.  The type std::vector<arma::mat> is meant to hold a sequence of\ndata sequences.  I think you are saying that if you are using 5 videos,\neach video is an individual arma::mat stored in the\nstd::vector<arma::mat>.  Then each point in the arma::mat should contain\none data point in each _column_ (not each row, which is what you wrote).\n\nI suspect that if you transposed your data matrices, it would work.  In\naddition to updating the documentation, I'll also add some runtime size\nchecks so that you'll get something more useful than \"Mat::operator():\nout of bounds\".\n\n>  I'm sorry to keep bothering you with what is probably obvious to\n>  someone working with hmm or machine learning techniques on a regular\n>  base, but as I mentioned before I'm an amateur when it comes to machine\n>  learning.\n\nThat's what this list is for. :)\n\nSometimes the hardest part of development is producing good\ndocumentation and often times that takes a lot of feedback; so I am very\nthankful that you've brought this up, because now it can be improved.\n\nThanks,\n\nRyan\n\n-- \nRyan Curtin       | \"Somebody dropped a bag on the sidewalk.\"\nryan@igglybob.com |   - Kit\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Wed, Apr 10, 2013 at 03:52:56PM +0800, Tommy Lin wrote:\n> Dear Sir/Madam,\n> \n> I am quite interested about the idea called improvement of tree travers.\n> \n> Could you please provide me with more information about it?\n\nHello Tommy,\n\nPlease ask a specific question and I can answer it.  This is too general\na question for me to effectively answer.  Also, the basic information on\nthe project is given on the Ideas page.\n\nThanks,\n\nRyan\n\n-- \nRyan Curtin       | \"You can think about it... but don't do it.\"\nryan@igglybob.com |   - Sheriff Justice\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Tue, Apr 09, 2013 at 02:02:24PM -0400, Ryan Curtin wrote:\n> I recently committed a fix which adds some small values to the\n> covariance matrices so that they don't end up being entirely\n> zero-valued.\n\nMy mistake.  I hadn't committed this fix until just now (r148883).\n\n-- \nRyan Curtin       | \"I am a meat popsicle.\"\nryan@igglybob.com |   - Korben Dallas \n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Wed, Apr 10, 2013 at 09:14:27AM +0200, Robert Pollak wrote:\n> Hello,\n> \n> is there an archive of this mailing list? The one on\n> https://mailman.cc.gatech.edu/pipermail/mlpack/ is empty.\n> \n> Btw., I can recommend setting one up at http://gmane.org/. I found their\n> search functionality useful, and one can also import a local (mbox)\n> archive to initialize it.\n\nI think that the archive got fubar'ed when the list was renamed.\nMailman gets picky sometimes.  I was planning to look into it today.  I\ndo have copies of all the messages sent to it, so if it comes to it, I\ncan manually rebuild it...\n\n-- \nRyan Curtin       | \"And the last thing I would ever do is lie to you.\"\nryan@igglybob.com |   - Marlon\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "Hello MLPack Team:\n                           I am a Graduate Student in Electrical\nEngineering at National University of Science and Technology,\nPakistan<http://www.nust.edu.pk>.\nI am deeply passionate about Machine Learning and have been doing a bunch\nof my own private projects. The fact that Google Summer of Code actually\ngives me an opportunity to be a part of a Team which is involved in stuff\nthat I love to do is fascinating.\n\n                           I am hugely interested in the Collaborative\nFiltering Package implementation for MLPack. Are there any particular\nalgorithms that MLPack team have in mind? Or if we can leverage the power\nof some other API, e.g., GraphLab, and try to integrate it with MLPack, is\nit something that can be considered? I will be grateful for a response.\n\n\nBest Regards,\nSalman Javaid\n<div dir=3D\"ltr\"><div><div>Hello MLPack Team:<br></div>=A0=A0=A0=A0=A0=A0=\n=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0 I am a Graduat=\ne Student in <span></span><span></span>Electrical Engineering at <a href=3D=\n\"http://www.nust.edu.pk\" target=3D\"_blank\">National University of Science a=\nnd Technology, Pakistan</a>.\n I am deeply passionate about Machine Learning and have been doing a=20\nbunch of my own private projects. The fact that Google Summer of Code=20\nactually gives me an opportunity to be a part of a Team which is=20\ninvolved in stuff that I love to do is fascinating.<br>\n\n<br></div>=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=\n=A0=A0=A0=A0=A0 I am hugely interested in the=20\nCollaborative Filtering Package implementation for MLPack. Are there any\n particular algorithms that MLPack team have in mind? Or if we can=20\nleverage the power of some other API, e.g., GraphLab, and try to=20\nintegrate it with MLPack, is it something that can be considered? I will\n be grateful for a response.<br>\n\n<span style=3D\"font-family:arial,sans-serif;font-size:13px;border-collapse:=\ncollapse;color:rgb(51,51,51)\"><div><br><br>Best Regards,<br></div></span><d=\niv><span style=3D\"font-family:arial,sans-serif;font-size:13px;border-collap=\nse:collapse;color:rgb(51,51,51)\">Salman Javaid<br>\n\n</span></div>\n</div>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "On Tue, Apr 09, 2013 at 01:34:15PM -0400, John Demme wrote:\n> Hi All-\n> \n> I'm trying to use mlpack's GMM with some data I've got. I'm not so familiar\n> with the statistical tools used here as I should be, so I've run into some\n> problems that I'm having trouble debugging on my own:\n> \n> - First, I often get \"error: inv(): matrix appears to be singular\" during\n> estimation. It appears that during estimation, one (or more) rows and\n> columns of a covariance matrix become 0, and I think this causes it to\n> become non-invertible.\n> \n> - Second, in cases when estimation completes, I often end up with means,\n> weights and covariances which are all -nan.\n> \n> I'm not sure whether I'm mis-using the tool or I've got funny data which\n> need to be conditioned. It's six-dimensional, values less than 1.0 and one\n> of the features is very often zero. (I'm wondering if that last bit means\n> that one good gaussian would be zero mean and zero stdev, resulting in a\n> degenerate covariance matrix -- though I don't know enough stat and linear\n> algebra to work this out.) Can someone give some advice?\n> \n> I've posted a small sub-set of my data which can trigger these problems:\n> www.cs.columbia.edu/~jdd/gmm_obs0.csv\n> \n> If I run \"./gmm -i gmm_obs0.csv -g 5\" I can get the first problem. Changing\n> the number of gaussians to 8 results in the second problem.\n\nHello John,\n\nMy own work has brought me back to GMMs in recent weeks and I have come\nacross the same issues you suggested.\n\nAre you using the svn trunk or mlpack 1.0.4?  If you aren't using trunk,\ntry doing that ('svn co http://svn.cc.gatech.edu/fastlab/mlpack/trunk\nmlpack').  I recently committed a fix which adds some small values to\nthe covariance matrices so that they don't end up being entirely\nzero-valued.  That situation arises when the initial clustering to set\nthe Gaussians returns a cluster with only one point in it (by default\nthis is K-Means).\n\nI'll try and hunt down the -nan issue, although it is worth pointing out\nthat the log-likelihood of the model is correctly -nan if there exist\npoints which are very far outside any of the Gaussians.  That is, if the\nprobability of an individual point is 0, then log(0) = -nan and the\nlog-likelihood of the whole model is -nan.  I'll let you know what I\nfind as I dig into it.  Thanks for attaching the test case -- this makes\ndebugging much easier.\n\nThanks for pointing out these issues.\n\nRyan\n\n-- \nRyan Curtin       | \"You got to stick with your principles.\"\nryan@igglybob.com |   - Harry Waters\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "Thanks for your help, Ryan!\n\nI'm testing out the latest now. Seems to fix at least one problem. It's\nlooking like fixing the other may just be a matter of adding trials until\nit finds something reasonable, though I'm not sure. I'll be looking at it\ncloser tomorrow.\n\n~John\n\n\nOn Tue, Apr 9, 2013 at 2:52 PM, Ryan Curtin <gth671b@mail.gatech.edu> wrote:\n\n> On Tue, Apr 09, 2013 at 02:14:09PM -0400, John Demme wrote:\n> > (missed reply-all a second ago)\n> >\n> > That patch is somewhat similar to a mod I tried. Mine helped sometimes,\n> but\n> > not in all situations. Both problems still occur with r14883:\n> >\n> > $ ./gmm -i gmm_obs0.csv -g 5\n> > [DEBUG] Compiled with debugging symbols.\n> > [DEBUG] Iterations: 42\n> > [DEBUG] EMFit::Estimate(): initial clustering log-likelihood: 1.4249e+06\n> >\n> > error: inv(): matrix appears to be singular\n> >\n> > terminate called after throwing an instance of 'std::runtime_error'\n> >   what():\n> > Aborted (core dumped)\n>\n> I have solved one of these, but I may not get to the second problem by\n> the end of the day.  r14884 commits the fix.\n>\n> The issue was not that the entire covariance matrix was zero but instead\n> that only one element on the diagonal was zero -- which makes it\n> non-invertible.  This could happen when the points for a particular\n> cluster have no variance in one dimension (in your case, it was the\n> fifth dimension).  So instead of checking if the sum of the covariance\n> diagonal is zero, now it checks each element and adds a perturbation\n> accordingly.\n>\n> The perturbation is small (1e-50) so it does not affect the results of\n> the phi() function to get the probability of a point.  At the same time,\n> it's large enough to make the matrix invertible.\n>\n> When I have a solution for the second one, I'll let you know.\n>\n> > On Tue, Apr 9, 2013 at 2:06 PM, Ryan Curtin <gth671b@mail.gatech.edu>\n> wrote:\n> > > (r148883)\n>\n> Oops, one too many 8s there.  I was thinking that number looked a little\n> large...\n>\n> Thanks for pointing these issues out.  :)\n>\n> Ryan\n>\n> --\n> Ryan Curtin       | \"Do I sound like I'm ordering a pizza?\"\n> ryan@igglybob.com |   - John McClane\n>\n<div dir=3D\"ltr\">Thanks for your help, Ryan!<div><br></div><div style>I&#39=\n;m testing out the latest now. Seems to fix at least one problem. It&#39;s =\nlooking like fixing the other may just be a matter of adding trials until i=\nt finds something reasonable, though I&#39;m not sure. I&#39;ll be looking =\nat it closer tomorrow.</div>\n\n<div style><br></div><div style>~John</div></div><div class=3D\"gmail_extra\"=\n><br><br><div class=3D\"gmail_quote\">On Tue, Apr 9, 2013 at 2:52 PM, Ryan Cu=\nrtin <span dir=3D\"ltr\">&lt;<a href=3D\"mailto:gth671b@mail.gatech.edu\" targe=\nt=3D\"_blank\">gth671b@mail.gatech.edu</a>&gt;</span> wrote:<br>\n\n<blockquote class=3D\"gmail_quote\" style=3D\"margin:0 0 0 .8ex;border-left:1p=\nx #ccc solid;padding-left:1ex\"><div class=3D\"im\">On Tue, Apr 09, 2013 at 02=\n:14:09PM -0400, John Demme wrote:<br>\n&gt; (missed reply-all a second ago)<br>\n&gt;<br>\n&gt; That patch is somewhat similar to a mod I tried. Mine helped sometimes=\n, but<br>\n&gt; not in all situations. Both problems still occur with r14883:<br>\n&gt;<br>\n&gt; $ ./gmm -i gmm_obs0.csv -g 5<br>\n&gt; [DEBUG] Compiled with debugging symbols.<br>\n&gt; [DEBUG] Iterations: 42<br>\n&gt; [DEBUG] EMFit::Estimate(): initial clustering log-likelihood: 1.4249e+=\n06<br>\n&gt;<br>\n&gt; error: inv(): matrix appears to be singular<br>\n&gt;<br>\n&gt; terminate called after throwing an instance of &#39;std::runtime_error=\n&#39;<br>\n&gt; =A0 what():<br>\n&gt; Aborted (core dumped)<br>\n<br>\n</div>I have solved one of these, but I may not get to the second problem b=\ny<br>\nthe end of the day. =A0r14884 commits the fix.<br>\n<br>\nThe issue was not that the entire covariance matrix was zero but instead<br=\n>\nthat only one element on the diagonal was zero -- which makes it<br>\nnon-invertible. =A0This could happen when the points for a particular<br>\ncluster have no variance in one dimension (in your case, it was the<br>\nfifth dimension). =A0So instead of checking if the sum of the covariance<br=\n>\ndiagonal is zero, now it checks each element and adds a perturbation<br>\naccordingly.<br>\n<br>\nThe perturbation is small (1e-50) so it does not affect the results of<br>\nthe phi() function to get the probability of a point. =A0At the same time,<=\nbr>\nit&#39;s large enough to make the matrix invertible.<br>\n<br>\nWhen I have a solution for the second one, I&#39;ll let you know.<br>\n<div class=3D\"im\"><br>\n&gt; On Tue, Apr 9, 2013 at 2:06 PM, Ryan Curtin &lt;<a href=3D\"mailto:gth6=\n71b@mail.gatech.edu\">gth671b@mail.gatech.edu</a>&gt; wrote:<br>\n</div>&gt; &gt; (r148883)<br>\n<br>\nOops, one too many 8s there. =A0I was thinking that number looked a little<=\nbr>\nlarge...<br>\n<br>\nThanks for pointing these issues out. =A0:)<br>\n<span class=3D\"HOEnZb\"><font color=3D\"#888888\"><br>\nRyan<br>\n<br>\n--<br>\nRyan Curtin =A0 =A0 =A0 | &quot;Do I sound like I&#39;m ordering a pizza?&q=\nuot;<br>\n<a href=3D\"mailto:ryan@igglybob.com\">ryan@igglybob.com</a> | =A0 - John McC=\nlane<br>\n</font></span></blockquote></div><br></div>\n", "Hello there,\n\nToday I heard back from Google and it turns out mlpack has been accepted\ninto GSoC 2013.  This means that Google will pay students to work on\nmlpack over the summer, and is a Very Good Thing (TM).\n\nIf you are interested in being a mentor for the project, you can sign up\nonline:\n\nhttp://www.google-melange.com/gsoc/org/google/gsoc2013/mlpack\n\nMentorship involves helping a student complete a project which they have\napplied to do, and to my understanding you will get a free t-shirt once\nit's all done.  Also, Google pays a stipend of $500 to us for each\naccepted student, which, because we don't really have any costs, can go\ndirectly to the mentor.  I need to look further into the precise details\nof that.\n\nUh, yeah, so: mentor a student, get $500 and maybe a t-shirt.\n\nThe steps to apply for mentorship should be on the webpage.  If you have\nany questions, feel free to email me.\n\nThe student questions should start rolling in shortly.  The Shogun\ndevelopers told me that they got somewhere around 100 applications last\nyear...\n\nRyan\n\n-- \nRyan Curtin       | \"Don't fight it son. Confess quickly! If you hold out too\nryan@igglybob.com | long you could jeopardize your credit rating.\"  - Guard\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "I am an undergraduate student at Indian Institute of Technology Roorkee,\nIndia\n\nI want to know about the possibility of project on adding new features to\nmlpack including Restricted Boltzmann Machines and some Genetic Algorithms\nranging from very basic like Simulated Annealing, (Mu,Lambda) Evolution\nStrategy to some of the more advanced algorithms.\n\nRegards,\nPramanshu Rajput\nIIT Roorkee\nIndia\n<div dir=3D\"ltr\"><span style=3D\"font-family:arial,sans-serif;font-size:13.3=\n33333969116211px\">I am an undergraduate student at Indian Institute of Tech=\nnology Roorkee, India</span><div style=3D\"font-family:arial,sans-serif;font=\n-size:13.333333969116211px\">\n<br></div><div style=3D\"font-family:arial,sans-serif;font-size:13.333333969=\n116211px\">I want to know about the possibility of project on adding new fea=\ntures to mlpack including Restricted Boltzmann Machines and some Genetic Al=\ngorithms ranging from very basic like Simulated Annealing, (Mu,Lambda) Evol=\nution Strategy to some of the more advanced algorithms.</div>\n<div style=3D\"font-family:arial,sans-serif;font-size:13.333333969116211px\">=\n<br></div><div style=3D\"font-family:arial,sans-serif;font-size:13.333333969=\n116211px\">Regards,</div><div style=3D\"font-family:arial,sans-serif;font-siz=\ne:13.333333969116211px\">\nPramanshu Rajput</div><div style=3D\"font-family:arial,sans-serif;font-size:=\n13.333333969116211px\">IIT Roorkee</div><div style=3D\"font-family:arial,sans=\n-serif;font-size:13.333333969116211px\">India</div></div>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "Dear Sir/Madam,\n\nI am quite interested about the idea called improvement of tree travers.\n\nCould you please provide me with more information about it?\n\nThanks a lot.\n\nRegards,\nTommy\n<div dir=3D\"ltr\">Dear Sir/Madam,<div><br></div><div style>I am quite intere=\nsted about the idea called improvement of tree travers.<br></div><div style=\n><br></div><div style>Could you please provide me with more information abo=\nut it?=A0</div>\n<div style><br></div><div style>Thanks a lot.</div><div style><br></div><di=\nv style>Regards,</div><div style>Tommy</div></div>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "Hello mlpack mailing list members,\n\n\n\nAs this is my first post to the mailing list, my appreciation to the\ndevelopers for creating the wonderful mlpack package.\n\n\n\nI have a question regarding mlpack::neighbor::NeighborSearch\n\n\n\nI am wondering if it is possible to perform a dynamic kNN search in\nmlpack efficiently solving the all-NN problem repeatedly with a\nchanging point set S.\n\n\n\nAs far as I know, all previously described methods handle only the\ninserting and deleting of points between queries or the case of points\nmoving along a predetermined trajectories. This is different from the\ndynamic problem I have in mind, wherein the points move slowly along\ntrajectories, but that these trajectories are not known in advance as\nin the case of an optimization problem.\n\n\n\nThe idea is that if the changes between point positions in subsequent\ncalls to the the mlpack::neighbor::AllkNN search algorithm are\nsufficiently small, updating the tree should be faster than rebuilding\nit from scratch. The mlpack algorithm would takes a S_i and a\ncorresponding tree T_i and update the tree to T_i+1 corresponding to a\nnew set of points S_i+1. A caveat is that the subsequent sets {S_i} and\n{S_i+1} will, in general, contain an similar, but not exactly the same,\nnumber of points.\n\n\n\nAny insight regarding this question would be most appreciated.\n\n\n\nAudrius Stundzia\n<!DOCTYPE html>\n<html>\n<head>\n<title></title>\n</head>\n<body><div>Hello mlpack mailing list members,&nbsp;<br></div>\n<div>&nbsp;</div>\n<div>As this is my first post to the mailing list, my appreciation to the developers for creating the wonderful mlpack package.<br></div>\n<div>&nbsp;</div>\n<div>I have a question regarding  mlpack::neighbor::NeighborSearch <br></div>\n<div>&nbsp;</div>\n<div>I am wondering if it is possible to perform a&nbsp;<i>dynamic</i>&nbsp;kNN search in mlpack efficiently solving the all-NN problem repeatedly with a changing point set S. <br></div>\n<div>&nbsp;</div>\n<div>As far as I know, all previously described methods handle only the inserting and deleting of points between queries or the case of points moving along a predetermined trajectories. This is different from the dynamic problem I have in mind, wherein the points move slowly along trajectories, but that these trajectories are not known in advance as in the case of an optimization problem.<br></div>\n<div>&nbsp;</div>\n<div>The idea is that if the changes between point positions in subsequent calls to the the mlpack::neighbor::AllkNN search algorithm are sufficiently small, updating the tree should be faster than rebuilding it from scratch. The mlpack algorithm would takes a S_i and a corresponding tree T_i and update the tree to T_i+1 corresponding to a new set of points S_i+1. A caveat is that the subsequent sets {S_i} and {S_i+1} will, in general, contain an similar, but not exactly the same, number of points.<br></div>\n<div>&nbsp;</div>\n<div>Any insight regarding this question would be most appreciated.<br></div>\n<div>&nbsp;</div>\n<div>Audrius Stundzia</div>\n<div>&nbsp;</div>\n<div>&nbsp;</div>\n</body>\n</html>\n", "On Tue, Apr 09, 2013 at 02:14:09PM -0400, John Demme wrote:\n> (missed reply-all a second ago)\n> \n> That patch is somewhat similar to a mod I tried. Mine helped sometimes, but\n> not in all situations. Both problems still occur with r14883:\n> \n> $ ./gmm -i gmm_obs0.csv -g 5\n> [DEBUG] Compiled with debugging symbols.\n> [DEBUG] Iterations: 42\n> [DEBUG] EMFit::Estimate(): initial clustering log-likelihood: 1.4249e+06\n> \n> error: inv(): matrix appears to be singular\n> \n> terminate called after throwing an instance of 'std::runtime_error'\n>   what():\n> Aborted (core dumped)\n\nI have solved one of these, but I may not get to the second problem by\nthe end of the day.  r14884 commits the fix.\n\nThe issue was not that the entire covariance matrix was zero but instead\nthat only one element on the diagonal was zero -- which makes it\nnon-invertible.  This could happen when the points for a particular\ncluster have no variance in one dimension (in your case, it was the\nfifth dimension).  So instead of checking if the sum of the covariance\ndiagonal is zero, now it checks each element and adds a perturbation\naccordingly.\n\nThe perturbation is small (1e-50) so it does not affect the results of\nthe phi() function to get the probability of a point.  At the same time,\nit's large enough to make the matrix invertible.\n\nWhen I have a solution for the second one, I'll let you know.\n\n> On Tue, Apr 9, 2013 at 2:06 PM, Ryan Curtin <gth671b@mail.gatech.edu> wrote:\n> > (r148883)\n\nOops, one too many 8s there.  I was thinking that number looked a little\nlarge...\n\nThanks for pointing these issues out.  :)\n\nRyan\n\n-- \nRyan Curtin       | \"Do I sound like I'm ordering a pizza?\"\nryan@igglybob.com |   - John McClane\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "Hi,\n\nI am currently pursuing MS by Research in the field of image processing and\nmachine learning at IIIT- Hyderabad, India. I am interested in working for\nmlpack this summer in a GSoC project.\n\nI propose to implement techniques for dimensionality reduction and metric\nlearning.\n\nhttp://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionality_Reduction.html\n\nI think this will be a good addition to mlpack.\n\nThere are also many kernel functions which have not been implemented.\n\nhttp://crsouza.blogspot.in/2010/03/kernel-functions-for-machine-learning.html\n\nWhat are your views about this?\n\nBest Regards,\nSiddharth\n\n-- \nLive as if you were to die tomorrow. Learn as if you were to live forever -\nM K Gandhi\n<div dir=3D\"ltr\"><div style=3D\"font-family:arial,sans-serif;font-size:13px\"=\n><div>Hi,<br><div><div><br></div><div>I am currently pursuing MS by Researc=\nh in the field of image processing and machine learning at IIIT- Hyderabad,=\n India. I am interested in working for mlpack this summer=A0in a GSoC=A0pro=\nject.<br>\n</div></div></div><div><br></div><div>I propose to implement techniques for=\n dimensionality reduction and metric learning.</div></div><div style=3D\"fon=\nt-family:arial,sans-serif;font-size:13px\"><br></div><div style=3D\"font-fami=\nly:arial,sans-serif;font-size:13px\">\n<a href=3D\"http://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionali=\nty_Reduction.html\" target=3D\"_blank\">http://homepage.tudelft.nl/19j49/Matla=\nb_Toolbox_for_Dimensionality_Reduction.html</a><br></div><div style=3D\"font=\n-family:arial,sans-serif;font-size:13px\">\n<br></div><div style=3D\"font-family:arial,sans-serif;font-size:13px\">I thin=\nk this will be a good addition to mlpack.</div><div style=3D\"font-family:ar=\nial,sans-serif;font-size:13px\"><br></div><div style=3D\"font-family:arial,sa=\nns-serif;font-size:13px\">\nThere are also many kernel functions which have not been implemented. =A0</=\ndiv><div style=3D\"font-family:arial,sans-serif;font-size:13px\"><br></div><d=\niv style=3D\"font-family:arial,sans-serif;font-size:13px\"><a href=3D\"http://=\ncrsouza.blogspot.in/2010/03/kernel-functions-for-machine-learning.html\" tar=\nget=3D\"_blank\">http://crsouza.blogspot.in/2010/03/kernel-functions-for-mach=\nine-learning.html</a><br>\n</div><div style=3D\"font-family:arial,sans-serif;font-size:13px\"><br></div>=\n<div style=3D\"font-family:arial,sans-serif;font-size:13px\">What are your vi=\news about this?</div><div style=3D\"font-family:arial,sans-serif;font-size:1=\n3px\">\n<br></div><div style=3D\"font-family:arial,sans-serif;font-size:13px\">Best R=\negards,</div><div style=3D\"font-family:arial,sans-serif;font-size:13px\">Sid=\ndharth</div><div><br></div>-- <br>Live as if you were to die tomorrow. Lear=\nn as if you were to live forever - M K Gandhi\n</div>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "Hello,\n\nI am a final year undergrad in  Computer Science at National Institute of\nTechnology, Surat. I am very interested in working on the Collaborative\nFiltering Package for mlpack.  I have previously done an internship in NLP\nat IISc, Bangalore and also a short workshop on Speech Technologies at\nCarnegie-Mellon - IIIT Delhi, winter school. In my final year project, I am\nworking on speech emotion recognition. These have mostly involved some form\nof Machine Learning - for instance SVMs in case of my current project. I\nwould really be enthusiastic to work hands on on implementing collaborative\nfiltering , though I don't have much  background in this particular area. I\nhave fairly good experience coding in C/C++ and python and I am eager to\nread up any background or preparatory material I find.\n\np.s. to know more of the projects I've worked on visit\nhttps://sites.google.com/site/madhuraparikh/\n\nRegards,\nMadhura Parikh\nIndia\n<div dir=3D\"ltr\"><div>Hello,<br><br></div>I am a final year undergrad in=A0=\n Computer Science at National Institute of Technology, Surat. I am very int=\nerested in working on the Collaborative Filtering Package for mlpack.=A0 I\n have previously done an internship in NLP at IISc, Bangalore and also a\n short workshop on Speech Technologies at Carnegie-Mellon - IIIT Delhi, win=\nter school. In my=20\nfinal year project, I am working on speech emotion recognition. These have =\nmostly involved some form of Machine Learning - for instance SVMs in case o=\nf my current project. I would really be enthusiastic to work hands on on im=\nplementing collaborative filtering , though I don&#39;t have much=A0 backgr=\nound in this particular area. I have fairly good experience coding in C/C++=\n and python and I am eager to read up any background or preparatory materia=\nl I find.<br>\n<br><div>p.s. to know more of the projects I&#39;ve worked on visit=A0<a hr=\nef=3D\"https://sites.google.com/site/madhuraparikh/\" target=3D\"_blank\">https=\n://sites.google.com/site/madhuraparikh/</a></div><div><br></div>\n<div>Regards,</div><div>Madhura Parikh<br></div><div>India<br></div></div>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "On Wed, Apr 10, 2013 at 10:34:56PM +0800, Tommy Lin wrote:\n> Although, it's may be ridiculous for a year 1 student to try such a\n> difficult project, I don't want to give up before trying.\n> \n> Coulu you please recommend some papers for me?\n\nThere are a number of other projects listed there which are not as\ndifficult and that don't require such specific domain knowledge.  Since\nyou have no knowledge of the field, I highly suggest you consider one of\nthe other ones.  Nevertheless, here are some references that you could\nlook into...\n\n----\n\nJ.H. Friedman, J.L. Bentley, and R.A. Finkel. \"An algorithm for finding\nbest matches in logarithmic expected time.\" ACM Transactions on\nMathematical Software (TOMS) 3.3 (1977): 209-226.\n\nJ.L. Bentley and J.H. Friedman. \"Data structures for range searching.\"\nACM Computing Surveys (CSUR) 11.4 (1979): 397-409.\n\nK.L. Clarkson. \"Nearest neighbor queries in metric spaces.\" Discrete &\nComputational Geometry 22.1 (1999): 63-93.\n\nA.W. Moore. \"The Anchors Hierarchy: using the triangle inequality to\nsurvive high-dimensional data.\"  Proceedings of the Sixteenth Conference\non Uncertainty in Artificial Intelligence (2000).\n\nA.G. Gray and A.W. Moore. \"N-body problems in statistical learning.\"\nAdvances in Neural Information Processing Systems (2001): 521-527.\n\nA.W. Moore.  \"Nonparametric density estimation: toward computational\ntractability.\"  Proceedings of the Third SIAM International Conference\non Data Mining (2003).\n\nA. Beygelzimer, S. Kakade, and J.L. Langford.  \"Cover trees for nearest\nneighbor.\"  Proceedings of the 23rd International Conference on Machine\nLearning (2006).\n\nP. Ram, D. Lee, W.B. March, A.G. Gray.  \"Linear-time algorithms for\npairwise statistical problems.\"  Advances in Neural Information\nProcessing Systems (2009).\n\n----\n\nI would not expect a first (or second) year undergraduate to be able to\neffectively tackle this project.  Instead a more likely candidate would\nbe someone near the completion of an undergraduate degree or a graduate\nstudent who is well-versed with data structures, low-level code\noptimization, template metaprogramming, and perhaps somewhat familiar\nwith some of the papers I listed above.\n\n-- \nRyan Curtin       | \"Like, with jetpacks?\"\nryan@igglybob.com |   - Scott\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Mon, Mar 11, 2013 at 11:10:56AM +0100, Jaelon wrote:\n> Dear Mrs, Sir,\n> \n> I have only recently begun learning about machine learning and more\n> specific about hidden markov models as I need to use these.\n> I have a project in C++ and the mlpack library seemed like the best\n> option to use for training and using HMM. However I find the\n> documentation about HMM somewhat vague on not entirely clear.\n> There is a piece of example code there but that doesn't work for me.\n> \n> Here is an overview of what I need to do:\n> I have a program that throws me data on which i need to train the\n> HMM. The data contains discrete as well as continuous data. The data\n> can vary between 4 and 27 variables being integers, floats and\n> vector<floats>. If I am correct these are my observations as I know\n> what these stand for and can extract them with ease.\n> But I don't need to use all of the different variables at once as I\n> want to look at the performance when using every one of them on its\n> own and then expanding it to take more variables into account.\n> For now I want to look at the first variable.\n> This is a sequence of integer values ranging from 0 to 5 (thus\n> discrete), these values are stored in a vector<int> . So here is\n> what i try to do:\n> \n> ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n> vector<arma::Mat<int> > handvector;\n> arma::Mat<int> handobservations;\n> vector<arma::Col<size_t> > states;\n> int hidden_states = 1; //leaving this on 1 for now\n> \n>  hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states,\n> distribution::DiscreteDistribution());\n>     for(uint i = 0; i<hands.size();i++)  //hands is a vector<int>\n> that is created earlier in the code\n>     {\n>         handobservations << hands[i] << arma::endr;\n>         handvector.push_back(handobservations);\n>         handobservations.clear();\n>     }\n> \n> handhmm.Train(handvector, states);\n> /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n> \n> Now the problem with this is the Train member function won't accept\n> vector<arma::Mat<int> >, it only seems to accept\n> vector<arma::Mat<double> >.\n> Also states is left empty for this and that isn't accepted either.\n> So what needs to be put in states and is there a way to make train\n> work with vector<arma::Mat<int> >?\n> \n> Any help or a more extended example than in the doc will be greatly\n> appreciated.\n\nHello Joery,\n\nThe Train() function accepts vector<arma::Mat<double> > regardless of\nthe distribution type.  For the discrete distribution, casting the\nintegers to doubles should work, but this is not an optimal solution:\n\n  arma::Mat<double> handobservations;\n  for (uint i = 0; i < hands.size(); i++)\n  {\n    handobservations << (double) hands[i] << arma::endr;\n    handvector.push_back(handobservations);\n    handobservations.clear();\n  }\n\nIn the coming days I will refactor the HMM class so that for\nDiscreteDistribution, arma::Mat<int> will be accepted (as it should be).\nThis will be slightly involved because it means each distribution type\nwill have to define its own observation type (arma::Mat<int> for\nDiscreteDistribution, arma::Mat<double> for GaussianDistribution, and so\nforth).\n\nI will also write more complete documentation and a tutorial for HMMs,\nand then I'll let you know when that is all done.  In the mean time, the\nfix I proposed above should work.  If it doesn't, or you have any other\nquestions, let me know.\n\n-- \nRyan Curtin       | \"Get off my lawn!\"\nryan@igglybob.com |   - Kowalski\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On 03/14/2013 07:43 PM, Ryan Curtin wrote:\n> On Thu, Mar 14, 2013 at 12:29:53PM +0100, Jaelon wrote:\n>> Hey,\n>>\n>>   I have yet another question about the hmm from mlpack, more precisely\n>>   about the syntax for observations.\n>>   I wil be using the \"Train (const std::vector< arma::mat > &dataSeq)\"\n>>   thus without the labeled data so the Baum-welch algorithm will be used.\n>>   But when using it with dataseq filled up with data that is observable i\n>>   get an error: Mat::operator(): out of bounds.\n>>   Here is what I put in there:\n>>     * I have a vectors of integers (which i convert to double for now)\n>>       for each timestamp, thus lets say one vector for each frame of a\n>>       video.\n>>     * These vectors are of variable length (1 or 2)\n>>     * There are multiple video's to train the hmm (obviously :p)\n>>     * Now I use the dataseq as a vector containing the data from all\n>>       videos. Thus lets say I use 5 videos, then the dataseq will have 5\n>>       columns whereby each column contains an arma::mat. This arma::mat\n>>       contains n rows (with n being the number of frames in the\n>>       corresponding video), each row in itself then contains the actual\n>>       vector data that I observed (thus 1 or 2 elements on each row).\n>>\n>>   Is this the way that it has to be used or am I using it wrong and do I\n>>   need to put the data per video in dataseq and thus execute the Train\n>>   method for each video.\n>>   I would like to have some more information about how to use the dataseq\n>>   in that function and hope that my explanation above is understandable\n>>   as it is kind of hard to explain.\n> The documentation here is somewhat unclear, and I will be fixing that\n> soon.  The type std::vector<arma::mat> is meant to hold a sequence of\n> data sequences.  I think you are saying that if you are using 5 videos,\n> each video is an individual arma::mat stored in the\n> std::vector<arma::mat>.  Then each point in the arma::mat should contain\n> one data point in each _column_ (not each row, which is what you wrote).\n>\n> I suspect that if you transposed your data matrices, it would work.  In\n> addition to updating the documentation, I'll also add some runtime size\n> checks so that you'll get something more useful than \"Mat::operator():\n> out of bounds\".\n>\n>>   I'm sorry to keep bothering you with what is probably obvious to\n>>   someone working with hmm or machine learning techniques on a regular\n>>   base, but as I mentioned before I'm an amateur when it comes to machine\n>>   learning.\n> That's what this list is for. :)\n>\n> Sometimes the hardest part of development is producing good\n> documentation and often times that takes a lot of feedback; so I am very\n> thankful that you've brought this up, because now it can be improved.\n>\n> Thanks,\n>\n> Ryan\n>\nMe again,\n\nI now understand how the observations are supposed to be filled in and \nchanged that in my code.\nAs you mentioned I just needed a transpose of the arma::mat matrices to \nhave the observation vector filled in correctly. However it is still \ngiving me the same error.\nNow I have tried to get it working with a simple piece of hardcoded data \nas follows:\n\nvector<arma::Mat<double> > handvector;\n     arma::Mat<double> observations;\n     int hidden_states = 5;\n     size_t observ_dimension;\n     vector<distribution::DiscreteDistribution> emissions;\n     arma::mat transitions;\n\n     observations << 5 << 4 << 5 << 5 << arma::endr;\n     observations << 2 << 1 << 1 << 2 << arma::endr;\n     handvector.push_back(observations);\n\n     hmm::HMM<distribution::DiscreteDistribution> handhmm(hidden_states, \ndistribution::DiscreteDistribution(observations.n_rows));\n\n     printf(\"Ready for training\\n\");\n     handhmm.Train(handvector); //the error comes from calling training \nwhen looking at my output\n     printf(\"Done training model\\n\");\n     observ_dimension = handhmm.Dimensionality();\n     printf(\"Retrieved dimensionality\\n\");\n     emissions = handhmm.Emission();\n     printf(\"Retrieved emission matrix\\n\");\n     transitions = handhmm.Transition();\n     printf(\"Retrieved transition matrix\");\n\nAs you see I used 4 columns with 2 datapoints for each observation while \nusing only one sequence.\nThis is a very simple example that is a fictional representation of what \nI put into the observations (when I would use 1 video with a very small \namount of extracted data).\nI initialize the hmm (handhmm) with 5 hidden states (this is just a \nrandom number I chose as the number of hidden states will be unknown in \nthe real case) and a discrete distribution for 2 observations.\n\nMaybe this example can clarify what I am doing and give you insight in \nwhat I might be doing wrong.\nIt think it might also be helpful to others to have such an example in \nthe documentation.\n\nWith kind regards,\nJoery\n<html>\n  <head>\n    <meta content=\"text/html; charset=ISO-8859-1\"\n      http-equiv=\"Content-Type\">\n  </head>\n  <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n    <div class=\"moz-cite-prefix\">On 03/14/2013 07:43 PM, Ryan Curtin\n      wrote:<br>\n    </div>\n    <blockquote cite=\"mid:20130314184303.GD21118@spoon.lugatgt.org\"\n      type=\"cite\">\n      <pre wrap=\"\">On Thu, Mar 14, 2013 at 12:29:53PM +0100, Jaelon wrote:\n</pre>\n      <blockquote type=\"cite\">\n        <pre wrap=\"\">Hey,\n\n I have yet another question about the hmm from mlpack, more precisely\n about the syntax for observations.\n I wil be using the \"Train (const std::vector&lt; arma::mat &gt; &amp;dataSeq)\"\n thus without the labeled data so the Baum-welch algorithm will be used.\n But when using it with dataseq filled up with data that is observable i\n get an error: Mat::operator(): out of bounds.\n Here is what I put in there:\n   * I have a vectors of integers (which i convert to double for now)\n     for each timestamp, thus lets say one vector for each frame of a\n     video.\n   * These vectors are of variable length (1 or 2)\n   * There are multiple video's to train the hmm (obviously :p)\n   * Now I use the dataseq as a vector containing the data from all\n     videos. Thus lets say I use 5 videos, then the dataseq will have 5\n     columns whereby each column contains an arma::mat. This arma::mat\n     contains n rows (with n being the number of frames in the\n     corresponding video), each row in itself then contains the actual\n     vector data that I observed (thus 1 or 2 elements on each row).\n\n Is this the way that it has to be used or am I using it wrong and do I\n need to put the data per video in dataseq and thus execute the Train\n method for each video.\n I would like to have some more information about how to use the dataseq\n in that function and hope that my explanation above is understandable\n as it is kind of hard to explain.\n</pre>\n      </blockquote>\n      <pre wrap=\"\">\nThe documentation here is somewhat unclear, and I will be fixing that\nsoon.  The type std::vector&lt;arma::mat&gt; is meant to hold a sequence of\ndata sequences.  I think you are saying that if you are using 5 videos,\neach video is an individual arma::mat stored in the\nstd::vector&lt;arma::mat&gt;.  Then each point in the arma::mat should contain\none data point in each _column_ (not each row, which is what you wrote).\n\nI suspect that if you transposed your data matrices, it would work.  In\naddition to updating the documentation, I'll also add some runtime size\nchecks so that you'll get something more useful than \"Mat::operator():\nout of bounds\".\n\n</pre>\n      <blockquote type=\"cite\">\n        <pre wrap=\"\"> I'm sorry to keep bothering you with what is probably obvious to\n someone working with hmm or machine learning techniques on a regular\n base, but as I mentioned before I'm an amateur when it comes to machine\n learning.\n</pre>\n      </blockquote>\n      <pre wrap=\"\">\nThat's what this list is for. :)\n\nSometimes the hardest part of development is producing good\ndocumentation and often times that takes a lot of feedback; so I am very\nthankful that you've brought this up, because now it can be improved.\n\nThanks,\n\nRyan\n\n</pre>\n    </blockquote>\n    Me again,<br>\n    <br>\n    I now understand how the observations are supposed to be filled in\n    and changed that in my code.<br>\n    As you mentioned I just needed a transpose of the arma::mat matrices\n    to have the observation vector filled in correctly. However it is\n    still giving me the same error.<br>\n    Now I have tried to get it working with a simple piece of hardcoded\n    data as follows:<br>\n    <br>\n    &nbsp;&nbsp;&nbsp; <font color=\"#000099\">vector&lt;arma::Mat&lt;double&gt; &gt;\n      handvector;<br>\n      &nbsp;&nbsp;&nbsp; arma::Mat&lt;double&gt; observations;<br>\n      &nbsp;&nbsp;&nbsp; int hidden_states = 5;<br>\n      &nbsp;&nbsp;&nbsp; size_t observ_dimension;<br>\n      &nbsp;&nbsp;&nbsp; vector&lt;distribution::DiscreteDistribution&gt; emissions;<br>\n      &nbsp;&nbsp;&nbsp; arma::mat transitions;<br>\n      <br>\n      &nbsp;&nbsp;&nbsp; observations &lt;&lt; 5 &lt;&lt; 4 &lt;&lt; 5 &lt;&lt; 5\n      &lt;&lt; arma::endr;<br>\n      &nbsp;&nbsp;&nbsp; observations &lt;&lt; 2 &lt;&lt; 1 &lt;&lt; 1 &lt;&lt; 2\n      &lt;&lt; arma::endr;<br>\n      &nbsp;&nbsp;&nbsp; handvector.push_back(observations);<br>\n      <br>\n      &nbsp;&nbsp;&nbsp; hmm::HMM&lt;distribution::DiscreteDistribution&gt;\n      handhmm(hidden_states,\n      distribution::DiscreteDistribution(observations.n_rows));<br>\n      <br>\n      &nbsp;&nbsp;&nbsp; printf(\"Ready for training\\n\");<br>\n      &nbsp;&nbsp;&nbsp; handhmm.Train(handvector); //the error comes from calling\n      training when looking at my output<br>\n      &nbsp;&nbsp;&nbsp; printf(\"Done training model\\n\");<br>\n      &nbsp;&nbsp;&nbsp; observ_dimension = handhmm.Dimensionality();<br>\n      &nbsp;&nbsp;&nbsp; printf(\"Retrieved dimensionality\\n\");<br>\n      &nbsp;&nbsp;&nbsp; emissions = handhmm.Emission();<br>\n      &nbsp;&nbsp;&nbsp; printf(\"Retrieved emission matrix\\n\");<br>\n      &nbsp;&nbsp;&nbsp; transitions = handhmm.Transition();<br>\n      &nbsp;&nbsp;&nbsp; printf(\"Retrieved transition matrix\");</font><br>\n    <br>\n    As you see I used 4 columns with 2 datapoints for each observation\n    while using only one sequence.<br>\n    This is a very simple example that is a fictional representation of\n    what I put into the observations (when I would use 1 video with a\n    very small amount of extracted data). <br>\n    I initialize the hmm (handhmm) with 5 hidden states (this is just a\n    random number I chose as the number of hidden states will be unknown\n    in the real case) and a discrete distribution for 2 observations.<br>\n    <br>\n    Maybe this example can clarify what I am doing and give you insight\n    in what I might be doing wrong.<br>\n    It think it might also be helpful to others to have such an example\n    in the documentation.<br>\n    <br>\n    With kind regards,<br>\n    Joery<br>\n  </body>\n</html>\n", "On 03/15/2013 11:11 AM, Jaelon wrote:\n> On 03/14/2013 07:43 PM, Ryan Curtin wrote:\n>> On Thu, Mar 14, 2013 at 12:29:53PM +0100, Jaelon wrote:\n>>> Hey,\n>>>\n>>>   I have yet another question about the hmm from mlpack, more precisely\n>>>   about the syntax for observations.\n>>>   I wil be using the \"Train (const std::vector< arma::mat > &dataSeq)\"\n>>>   thus without the labeled data so the Baum-welch algorithm will be used.\n>>>   But when using it with dataseq filled up with data that is observable i\n>>>   get an error: Mat::operator(): out of bounds.\n>>>   Here is what I put in there:\n>>>     * I have a vectors of integers (which i convert to double for now)\n>>>       for each timestamp, thus lets say one vector for each frame of a\n>>>       video.\n>>>     * These vectors are of variable length (1 or 2)\n>>>     * There are multiple video's to train the hmm (obviously :p)\n>>>     * Now I use the dataseq as a vector containing the data from all\n>>>       videos. Thus lets say I use 5 videos, then the dataseq will have 5\n>>>       columns whereby each column contains an arma::mat. This arma::mat\n>>>       contains n rows (with n being the number of frames in the\n>>>       corresponding video), each row in itself then contains the actual\n>>>       vector data that I observed (thus 1 or 2 elements on each row).\n>>>\n>>>   Is this the way that it has to be used or am I using it wrong and do I\n>>>   need to put the data per video in dataseq and thus execute the Train\n>>>   method for each video.\n>>>   I would like to have some more information about how to use the dataseq\n>>>   in that function and hope that my explanation above is understandable\n>>>   as it is kind of hard to explain.\n>> The documentation here is somewhat unclear, and I will be fixing that\n>> soon.  The type std::vector<arma::mat> is meant to hold a sequence of\n>> data sequences.  I think you are saying that if you are using 5 videos,\n>> each video is an individual arma::mat stored in the\n>> std::vector<arma::mat>.  Then each point in the arma::mat should contain\n>> one data point in each _column_ (not each row, which is what you wrote).\n>>\n>> I suspect that if you transposed your data matrices, it would work.  In\n>> addition to updating the documentation, I'll also add some runtime size\n>> checks so that you'll get something more useful than \"Mat::operator():\n>> out of bounds\".\n>>\n>>>   I'm sorry to keep bothering you with what is probably obvious to\n>>>   someone working with hmm or machine learning techniques on a regular\n>>>   base, but as I mentioned before I'm an amateur when it comes to machine\n>>>   learning.\n>> That's what this list is for. :)\n>>\n>> Sometimes the hardest part of development is producing good\n>> documentation and often times that takes a lot of feedback; so I am very\n>> thankful that you've brought this up, because now it can be improved.\n>>\n>> Thanks,\n>>\n>> Ryan\n>>\n> Me again,\n>\n> I now understand how the observations are supposed to be filled in and \n> changed that in my code.\n> As you mentioned I just needed a transpose of the arma::mat matrices \n> to have the observation vector filled in correctly. However it is \n> still giving me the same error.\n> Now I have tried to get it working with a simple piece of hardcoded \n> data as follows:\n>\n> vector<arma::Mat<double> > handvector;\n>     arma::Mat<double> observations;\n>     int hidden_states = 5;\n>     size_t observ_dimension;\n>     vector<distribution::DiscreteDistribution> emissions;\n>     arma::mat transitions;\n>\n>     observations << 5 << 4 << 5 << 5 << arma::endr;\n>     observations << 2 << 1 << 1 << 2 << arma::endr;\n>     handvector.push_back(observations);\n>\n>     hmm::HMM<distribution::DiscreteDistribution> \n> handhmm(hidden_states, \n> distribution::DiscreteDistribution(observations.n_rows));\n>\n>     printf(\"Ready for training\\n\");\n>     handhmm.Train(handvector); //the error comes from calling training \n> when looking at my output\n>     printf(\"Done training model\\n\");\n>     observ_dimension = handhmm.Dimensionality();\n>     printf(\"Retrieved dimensionality\\n\");\n>     emissions = handhmm.Emission();\n>     printf(\"Retrieved emission matrix\\n\");\n>     transitions = handhmm.Transition();\n>     printf(\"Retrieved transition matrix\");\n>\n> As you see I used 4 columns with 2 datapoints for each observation \n> while using only one sequence.\n> This is a very simple example that is a fictional representation of \n> what I put into the observations (when I would use 1 video with a very \n> small amount of extracted data).\n> I initialize the hmm (handhmm) with 5 hidden states (this is just a \n> random number I chose as the number of hidden states will be unknown \n> in the real case) and a discrete distribution for 2 observations.\n>\n> Maybe this example can clarify what I am doing and give you insight in \n> what I might be doing wrong.\n> It think it might also be helpful to others to have such an example in \n> the documentation.\n>\n> With kind regards,\n> Joery\nEdit: I have also figured out that\nobservations << 5 << 4 << 5 << 5 << arma::endr;\nobservations << 2 << 1 << 1 << 2 << arma::endr;\nwrites to the first row 2 times so I changed that to\nobservations << 5 << 4 << 5 << 5 << arma::endr << 2 << 1 << 1 << 2 << \narma::endr;\nHowever that doesn't fix the problem (only brought to my intention that \nit is better to just initialize the arma::mat with a given size and then \nsetting the elements individually).\n<html>\n  <head>\n    <meta content=\"text/html; charset=ISO-8859-1\"\n      http-equiv=\"Content-Type\">\n  </head>\n  <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n    <div class=\"moz-cite-prefix\">On 03/15/2013 11:11 AM, Jaelon wrote:<br>\n    </div>\n    <blockquote cite=\"mid:5142F3BA.9070902@hotmail.com\" type=\"cite\">\n      <meta content=\"text/html; charset=ISO-8859-1\"\n        http-equiv=\"Content-Type\">\n      <div class=\"moz-cite-prefix\">On 03/14/2013 07:43 PM, Ryan Curtin\n        wrote:<br>\n      </div>\n      <blockquote cite=\"mid:20130314184303.GD21118@spoon.lugatgt.org\"\n        type=\"cite\">\n        <pre wrap=\"\">On Thu, Mar 14, 2013 at 12:29:53PM +0100, Jaelon wrote:\n</pre>\n        <blockquote type=\"cite\">\n          <pre wrap=\"\">Hey,\n\n I have yet another question about the hmm from mlpack, more precisely\n about the syntax for observations.\n I wil be using the \"Train (const std::vector&lt; arma::mat &gt; &amp;dataSeq)\"\n thus without the labeled data so the Baum-welch algorithm will be used.\n But when using it with dataseq filled up with data that is observable i\n get an error: Mat::operator(): out of bounds.\n Here is what I put in there:\n   * I have a vectors of integers (which i convert to double for now)\n     for each timestamp, thus lets say one vector for each frame of a\n     video.\n   * These vectors are of variable length (1 or 2)\n   * There are multiple video's to train the hmm (obviously :p)\n   * Now I use the dataseq as a vector containing the data from all\n     videos. Thus lets say I use 5 videos, then the dataseq will have 5\n     columns whereby each column contains an arma::mat. This arma::mat\n     contains n rows (with n being the number of frames in the\n     corresponding video), each row in itself then contains the actual\n     vector data that I observed (thus 1 or 2 elements on each row).\n\n Is this the way that it has to be used or am I using it wrong and do I\n need to put the data per video in dataseq and thus execute the Train\n method for each video.\n I would like to have some more information about how to use the dataseq\n in that function and hope that my explanation above is understandable\n as it is kind of hard to explain.\n</pre>\n        </blockquote>\n        <pre wrap=\"\">The documentation here is somewhat unclear, and I will be fixing that\nsoon.  The type std::vector&lt;arma::mat&gt; is meant to hold a sequence of\ndata sequences.  I think you are saying that if you are using 5 videos,\neach video is an individual arma::mat stored in the\nstd::vector&lt;arma::mat&gt;.  Then each point in the arma::mat should contain\none data point in each _column_ (not each row, which is what you wrote).\n\nI suspect that if you transposed your data matrices, it would work.  In\naddition to updating the documentation, I'll also add some runtime size\nchecks so that you'll get something more useful than \"Mat::operator():\nout of bounds\".\n\n</pre>\n        <blockquote type=\"cite\">\n          <pre wrap=\"\"> I'm sorry to keep bothering you with what is probably obvious to\n someone working with hmm or machine learning techniques on a regular\n base, but as I mentioned before I'm an amateur when it comes to machine\n learning.\n</pre>\n        </blockquote>\n        <pre wrap=\"\">That's what this list is for. :)\n\nSometimes the hardest part of development is producing good\ndocumentation and often times that takes a lot of feedback; so I am very\nthankful that you've brought this up, because now it can be improved.\n\nThanks,\n\nRyan\n\n</pre>\n      </blockquote>\n      Me again,<br>\n      <br>\n      I now understand how the observations are supposed to be filled in\n      and changed that in my code.<br>\n      As you mentioned I just needed a transpose of the arma::mat\n      matrices to have the observation vector filled in correctly.\n      However it is still giving me the same error.<br>\n      Now I have tried to get it working with a simple piece of\n      hardcoded data as follows:<br>\n      <br>\n      &nbsp;&nbsp;&nbsp; <font color=\"#000099\">vector&lt;arma::Mat&lt;double&gt; &gt;\n        handvector;<br>\n        &nbsp;&nbsp;&nbsp; arma::Mat&lt;double&gt; observations;<br>\n        &nbsp;&nbsp;&nbsp; int hidden_states = 5;<br>\n        &nbsp;&nbsp;&nbsp; size_t observ_dimension;<br>\n        &nbsp;&nbsp;&nbsp; vector&lt;distribution::DiscreteDistribution&gt; emissions;<br>\n        &nbsp;&nbsp;&nbsp; arma::mat transitions;<br>\n        <br>\n        &nbsp;&nbsp;&nbsp; observations &lt;&lt; 5 &lt;&lt; 4 &lt;&lt; 5 &lt;&lt; 5\n        &lt;&lt; arma::endr;<br>\n        &nbsp;&nbsp;&nbsp; observations &lt;&lt; 2 &lt;&lt; 1 &lt;&lt; 1 &lt;&lt; 2\n        &lt;&lt; arma::endr;<br>\n        &nbsp;&nbsp;&nbsp; handvector.push_back(observations);<br>\n        <br>\n        &nbsp;&nbsp;&nbsp; hmm::HMM&lt;distribution::DiscreteDistribution&gt;\n        handhmm(hidden_states,\n        distribution::DiscreteDistribution(observations.n_rows));<br>\n        <br>\n        &nbsp;&nbsp;&nbsp; printf(\"Ready for training\\n\");<br>\n        &nbsp;&nbsp;&nbsp; handhmm.Train(handvector); //the error comes from calling\n        training when looking at my output<br>\n        &nbsp;&nbsp;&nbsp; printf(\"Done training model\\n\");<br>\n        &nbsp;&nbsp;&nbsp; observ_dimension = handhmm.Dimensionality();<br>\n        &nbsp;&nbsp;&nbsp; printf(\"Retrieved dimensionality\\n\");<br>\n        &nbsp;&nbsp;&nbsp; emissions = handhmm.Emission();<br>\n        &nbsp;&nbsp;&nbsp; printf(\"Retrieved emission matrix\\n\");<br>\n        &nbsp;&nbsp;&nbsp; transitions = handhmm.Transition();<br>\n        &nbsp;&nbsp;&nbsp; printf(\"Retrieved transition matrix\");</font><br>\n      <br>\n      As you see I used 4 columns with 2 datapoints for each observation\n      while using only one sequence.<br>\n      This is a very simple example that is a fictional representation\n      of what I put into the observations (when I would use 1 video with\n      a very small amount of extracted data). <br>\n      I initialize the hmm (handhmm) with 5 hidden states (this is just\n      a random number I chose as the number of hidden states will be\n      unknown in the real case) and a discrete distribution for 2\n      observations.<br>\n      <br>\n      Maybe this example can clarify what I am doing and give you\n      insight in what I might be doing wrong.<br>\n      It think it might also be helpful to others to have such an\n      example in the documentation.<br>\n      <br>\n      With kind regards,<br>\n      Joery<br>\n    </blockquote>\n    Edit: I have also figured out that <br>\n    <font color=\"#000099\">observations &lt;&lt; 5 &lt;&lt; 4 &lt;&lt; 5\n      &lt;&lt; 5 &lt;&lt; arma::endr;<br>\n      observations &lt;&lt; 2 &lt;&lt; 1 &lt;&lt; 1 &lt;&lt; 2 &lt;&lt;\n      arma::endr;</font><br>\n    writes to the first row 2 times so I changed that to <br>\n    <font color=\"#000099\">observations &lt;&lt; 5 &lt;&lt; 4 &lt;&lt; 5\n      &lt;&lt; 5 &lt;&lt; arma::endr &lt;&lt; 2 &lt;&lt; 1 &lt;&lt; 1\n      &lt;&lt; 2 &lt;&lt; arma::endr;</font><br>\n    However that doesn't fix the problem (only brought to my intention\n    that it is better to just initialize the arma::mat with a given size\n    and then setting the elements individually).<br>\n  </body>\n</html>\n", "Hey,\n\nI'm a student in my final year and ran in the exact same problem yesterday.\nI have explained this to my mentor and he told me that it would prob be \nthe excessive zero values in my training set.\nHis suggestion was to replace the zero values with a higher and \nextremely unlikely number with some random noise on it. Although his \nidea sounds interesting and could work I haven't had time to try it out \nand think it depends on what you are trying to do whether it will work \nor not.\nIn my case i have data in 15 dimensions but not every dimension will \nhave a value available all the time so I keep them at zero in that case. \nIn this case it might just work to give them a high value.\n\nBut perhaps someone with more experience has a better and definite \nsolution for the problem at hand.\n\nGood luck,\nJoery\n\nOn 04/09/2013 07:34 PM, John Demme wrote:\n> Hi All-\n>\n> I'm trying to use mlpack's GMM with some data I've got. I'm not so \n> familiar with the statistical tools used here as I should be, so I've \n> run into some problems that I'm having trouble debugging on my own:\n>\n> - First, I often get \"error: inv(): matrix appears to be singular\" \n> during estimation. It appears that during estimation, one (or more) \n> rows and columns of a covariance matrix become 0, and I think this \n> causes it to become non-invertible.\n>\n> - Second, in cases when estimation completes, I often end up with \n> means, weights and covariances which are all -nan.\n>\n> I'm not sure whether I'm mis-using the tool or I've got funny data \n> which need to be conditioned. It's six-dimensional, values less than \n> 1.0 and one of the features is very often zero. (I'm wondering if that \n> last bit means that one good gaussian would be zero mean and zero \n> stdev, resulting in a degenerate covariance matrix -- though I don't \n> know enough stat and linear algebra to work this out.) Can someone \n> give some advice?\n>\n> I've posted a small sub-set of my data which can trigger these problems:\n> www.cs.columbia.edu/~jdd/gmm_obs0.csv \n> <http://www.cs.columbia.edu/%7Ejdd/gmm_obs0.csv>\n>\n> If I run \"./gmm -i gmm_obs0.csv -g 5\" I can get the first problem. \n> Changing the number of gaussians to 8 results in the second problem.\n>\n> Thanks in advance,\n> John\n>\n>\n> _______________________________________________\n> mlpack mailing list\n> mlpack@cc.gatech.edu\n> https://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n<html>\n  <head>\n    <meta content=\"text/html; charset=ISO-8859-1\"\n      http-equiv=\"Content-Type\">\n  </head>\n  <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n    Hey,<br>\n    <br>\n    I'm a student in my final year and ran in the exact same problem\n    yesterday.<br>\n    I have explained this to my mentor and he told me that it would prob\n    be the excessive zero values in my training set. <br>\n    His suggestion was to replace the zero values with a higher and\n    extremely unlikely number with some random noise on it. Although his\n    idea sounds interesting and could work I haven't had time to try it\n    out and think it depends on what you are trying to do whether it\n    will work or not. <br>\n    In my case i have data in 15 dimensions but not every dimension will\n    have a value available all the time so I keep them at zero in that\n    case. In this case it might just work to give them a high value.<br>\n    <br>\n    But perhaps someone with more experience has a better and definite\n    solution for the problem at hand.<br>\n    <br>\n    Good luck,<br>\n    Joery<br>\n    <br>\n    <div class=\"moz-cite-prefix\">On 04/09/2013 07:34 PM, John Demme\n      wrote:<br>\n    </div>\n    <blockquote\ncite=\"mid:CAOjmg=uitPrYaFJskDpBzzMVJxz+ktS1_tbkZxjst2siaEzy_A@mail.gmail.com\"\n      type=\"cite\">\n      <div dir=\"ltr\">\n        <div style=\"\">Hi All-</div>\n        <div style=\"\"><br>\n        </div>\n        <div style=\"\">I'm trying to use mlpack's GMM with some data I've\n          got. I'm not so familiar with the statistical tools used here\n          as I should be, so I've run into some problems that I'm having\n          trouble debugging on my own:</div>\n        <div style=\"\"><br>\n        </div>\n        <div style=\"\">- First, I often get \"error: inv(): matrix appears\n          to be singular\" during estimation. It appears that during\n          estimation, one (or more) rows and columns of a covariance\n          matrix become 0, and I think this causes it to become\n          non-invertible.</div>\n        <div style=\"\"><br>\n        </div>\n        <div style=\"\">- Second, in cases when estimation completes, I\n          often end up with means, weights and covariances which are all\n          -nan.</div>\n        <div style=\"\"><br>\n        </div>\n        <div style=\"\">I'm not sure whether I'm mis-using the tool or\n          I've got funny data which need to be conditioned. It's\n          six-dimensional, values less than 1.0 and one of the features\n          is very often zero. (I'm wondering if that last bit means that\n          one good gaussian would be zero mean and zero stdev, resulting\n          in a degenerate covariance matrix -- though I don't know\n          enough stat and linear algebra to work this out.) Can someone\n          give some advice?</div>\n        <div style=\"\"><br>\n        </div>\n        <div style=\"\">I've posted a small sub-set of my data which can\n          trigger these problems:</div>\n        <a moz-do-not-send=\"true\"\n          href=\"http://www.cs.columbia.edu/%7Ejdd/gmm_obs0.csv\">www.cs.columbia.edu/~jdd/gmm_obs0.csv</a><br>\n        <div><br>\n        </div>\n        <div style=\"\">If I run \"./gmm -i gmm_obs0.csv -g 5\" I can get\n          the first problem. Changing the number of gaussians to 8\n          results in the second problem.</div>\n        <div style=\"\"><br>\n        </div>\n        <div style=\"\">Thanks in advance,</div>\n        <div style=\"\">John<br>\n        </div>\n      </div>\n      <br>\n      <fieldset class=\"mimeAttachmentHeader\"></fieldset>\n      <br>\n      <pre wrap=\"\">_______________________________________________\nmlpack mailing list\n<a class=\"moz-txt-link-abbreviated\" href=\"mailto:mlpack@cc.gatech.edu\">mlpack@cc.gatech.edu</a>\n<a class=\"moz-txt-link-freetext\" href=\"https://mailman.cc.gatech.edu/mailman/listinfo/mlpack\">https://mailman.cc.gatech.edu/mailman/listinfo/mlpack</a>\n</pre>\n    </blockquote>\n    <br>\n  </body>\n</html>\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n", "On Tue, Mar 19, 2013 at 08:30:27PM +0100, Jaelon wrote:\n> One more question now though: Is the download on the website the one\n> with the updated Distribution code, as it states \"released februari\n> 8\"\n\nSorry for the slow response -- I was out of town for a while.  The\nupdated Distribution code is available from subversion, so you'll have\nto get it using svn:\n\n$ svn co http://svn.cc.gatech.edu/fastlab/mlpack/trunk/ mlpack\n\nThe next mlpack release, which will probably be in a month or two, will\ninclude those changes, but for now, you can just use what's in the\nsubversion repositories.\n\n-- \nRyan Curtin       | \"I am a golden god!\"\nryan@igglybob.com |   - Russell\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Wed, Apr 10, 2013 at 08:49:12AM +0530, Siddharth wrote:\n> Hi,\n> \n> I am currently pursuing MS by Research in the field of image processing and\n> machine learning at IIIT- Hyderabad, India. I am interested in working for\n> mlpack this summer in a GSoC project.\n> \n> I propose to implement techniques for dimensionality reduction and metric\n> learning.\n> \n> http://homepage.tudelft.nl/19j49/Matlab_Toolbox_for_Dimensionality_Reduction.html\n> \n> I think this will be a good addition to mlpack.\n\nWhich techniques are you planning to implement?  Many of those nonlinear\ndimensionality techniques are slow.  Instead what would be more\ninteresting is a working implementation of MVU with LRSDP (see the end\nof the ideas list).\n\nThere is no purpose in mlpack implementing a variety of methods poorly.\nBreadth is important but it cannot come at the cost of quality.\n\nIf you are going to propose implementing all (or even some) (or more\nthan one) of these methods, you will have to convince us that you can\nimplement all of them effectively.  And with tests.  Proper unit tests\nare very important, and, testing machine learning methods for accuracy\n-- especially when most of the \"tests\" available are just reference\nimplementations which could contain bugs -- is a rather difficult task\nwhich can take far longer than implementing the methods itself.\n\nAlso, if you were planning to just wrap the MATLAB functions or\n\"translate\" them to C++, that's not acceptable.  The implementation must\nbe from the ground up and needs to be provably faster than any existing\nimplementations to be accepted into the mlpack codebase (of course, you\ncan't know that when you propose the project, but if you have papers\nwhich detail faster algorithms to implement, those are a good start).\n\n> There are also many kernel functions which have not been implemented.\n> \n> http://crsouza.blogspot.in/2010/03/kernel-functions-for-machine-learning.html\n\nI could implement these, fully with tests, in probably a day and a half.\nThis could be an addendum onto an existing project but is too simple for\nsomeone with knowledge as advanced as yourself.\n\nHopefully these are helpful answers.  Let me know if I can clarify\nfurther. \n\n-- \nRyan Curtin       | \"Gentlemen, you can't fight in here!  This is the\nryan@igglybob.com | War Room!\" - President Muffley\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n\n", "On Tue, Apr 09, 2013 at 03:58:06PM +0200, Robert Pollak wrote:\n> Dear mlpack developers,\n> \n> the OpenCV Machine Learning Library [1] is another location where C++\n> implementations of machine learning methods are collected. How do you\n> position yourself with respect to that library?\n> \n> E.g., [2] gives the impression that you are not going to wrap 3rd party\n> methods, like libsvm (as OpenCV does). Is this correct?\n\nHello Robert,\n\nOpenCV focuses on computer vision (hence the name) whereas mlpack\nfocuses on large-scale statistical learning techniques.  mlpack is more\napplicable to people who are either looking to try less standard machine\nlearning algorithms (i.e. sparse coding, LARS, density estimation trees,\netc.) or who are looking to modify our flexible algorithms for the\npurposes of their research.  To that end, for instance, any user can\nwrite their own distance metric and plug it in to mlpack code (without\nany runtime penalty).\n\nYou are right that we aren't very interested in wrapping other libraries\n(other than Armadillo).  Some libraries -- especially scientific\nlibraries (cough cough Boost.math) -- change APIs in unexpected ways and\nthis makes maintenance an ugly proposition filled with #ifdefs and other\nhackery.  Instead, our goal is to, at the machine learning method level,\nkeep things in-house for the sake of maintainability.\n\nWe'd love to have an SVM implementation someday, but there isn't too\nmuch point in producing one if it can't outperform libsvm (if it can't,\nthere's not too much motivation to use our implementation).\n\nDoes this clarify things?  If not, let me know.\n\nThanks,\n\nRyan\n\n-- \nRyan Curtin       | \"Do they hurt?\"\nryan@igglybob.com |   - Jessica 6\n_______________________________________________\nmlpack mailing list\nmlpack@cc.gatech.edu\nhttps://mailman.cc.gatech.edu/mailman/listinfo/mlpack\n"]